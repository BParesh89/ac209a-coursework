{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/AC 209A/STAT 121A Data Science: Midterm 2\n",
    "**Harvard University**<br>\n",
    "**Fall 2016**<br>\n",
    "**Instructors: W. Pan, P. Protopapas, K. Rader**<br>\n",
    "**Due Date: ** Tuesday, November 22nd, 2016 at 12:00pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear namespace\n",
    "for name in dir():\n",
    "    if not name.startswith('_'):\n",
    "        del globals()[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ploting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scientific computing\n",
    "import scipy as sp\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Other\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Diagnosing the Semian Flu 2016\n",
    "\n",
    "You are given the early data for an outbreak of a dangerous virus originating from a group of primates being keeped in a Massechussetts biomedical research lab, this virus is dubbed the \"Semian Flu\".\n",
    "\n",
    "You have the medical records of $n$ number of patients in `'flu_train.csv`. There are two general types of patients in the data, flu patients and healthy (this is recorded in the column labeled `flu`, a 0 indicates the absences of the virus and a 1 indicates presence). Furthermore, scientists have found that there are two strains of the virus, each requiring a different type of treatment (this is recorded in the column labeled `flutype`, a 1 indicates the absences of the virus, a 2 indicates presence of strain 1 and a 3 indicates the presence of strain 2).\n",
    "\n",
    "**Your task:** build a model to predict if a given patient has the flu. Your goal is to catch as many flu patients as possible without misdiagnosing too many healthy patients.\n",
    "\n",
    "**The deliverable:** a function called `flu_predict` which satisfies:\n",
    "\n",
    "- input: `x_test`, a set of medical predictors for a group of patients\n",
    "- output: `y_pred`, a set of labels, one for each patient; 0 for healthy and 1 for infected with the flu virus\n",
    "\n",
    "The MA state government will use your model to diagnose sets of future patients (held by us). You can expect that there will be an increase in the number of flu patients in any groups of patients in the future.\n",
    "\n",
    "We provide you with some benchmarks for comparison.\n",
    "\n",
    "**Baseline Model:** \n",
    "- ~50% expected accuracy on healthy patients in observed data\n",
    "- ~50% expected accuracy on flu patients in observed data\n",
    "- ~50% expected accuracy on healthy patients in future data \n",
    "- ~50% expected accuracy on flu patients in future data\n",
    "- time to build: 5 min\n",
    "\n",
    "**Reasonable Model:** \n",
    "- ~69% expected accuracy on healthy patients in observed data\n",
    "- ~55% expected accuracy on flu patients, in observed data\n",
    "- ~69% expected accuracy on healthy patients in future data\n",
    "- ~60% expected accuracy on flu patients, in future data\n",
    "- time to build: 20 min\n",
    "\n",
    "**Grading:**\n",
    "Your grade will be based on:\n",
    "1. your model's ability to out-perform our benchmarks\n",
    "2. your ability to carefully and thoroughly follow the data science pipeline (see lecture slides for definition)\n",
    "3. the extend to which all choices are reasonable and defensible by methods you have learned in this class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions (necessary for the following calculations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  table\n",
    "# A function that is similar to the R table function\n",
    "# Input: \n",
    "#      x (x values)\n",
    "# Returns: \n",
    "#      table (table)\n",
    "\n",
    "def table(x):\n",
    "    table = pd.DataFrame(x.value_counts(dropna=True))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load business data\n",
    "df_flu = pd.read_csv('datasets/flu_train.csv')\n",
    "df_flu_fin = pd.read_csv('datasets/flu_test_no_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (5246, 76)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>AgeDecade</th>\n",
       "      <th>AgeMonths</th>\n",
       "      <th>Race1</th>\n",
       "      <th>Race3</th>\n",
       "      <th>Education</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>HHIncome</th>\n",
       "      <th>...</th>\n",
       "      <th>HardDrugs</th>\n",
       "      <th>SexEver</th>\n",
       "      <th>SexAge</th>\n",
       "      <th>SexNumPartnLife</th>\n",
       "      <th>SexNumPartYear</th>\n",
       "      <th>SameSex</th>\n",
       "      <th>SexOrientation</th>\n",
       "      <th>PregnantNow</th>\n",
       "      <th>flu</th>\n",
       "      <th>flutype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51624</td>\n",
       "      <td>male</td>\n",
       "      <td>34</td>\n",
       "      <td>30-39</td>\n",
       "      <td>409.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High School</td>\n",
       "      <td>Married</td>\n",
       "      <td>25000-34999</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51630</td>\n",
       "      <td>female</td>\n",
       "      <td>49</td>\n",
       "      <td>40-49</td>\n",
       "      <td>596.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some College</td>\n",
       "      <td>LivePartner</td>\n",
       "      <td>35000-44999</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51638</td>\n",
       "      <td>male</td>\n",
       "      <td>9</td>\n",
       "      <td>0-9</td>\n",
       "      <td>115.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75000-99999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51646</td>\n",
       "      <td>male</td>\n",
       "      <td>8</td>\n",
       "      <td>0-9</td>\n",
       "      <td>101.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55000-64999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51647</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>40-49</td>\n",
       "      <td>541.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>College Grad</td>\n",
       "      <td>Married</td>\n",
       "      <td>75000-99999</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bisexual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Gender  Age AgeDecade  AgeMonths  Race1 Race3     Education  \\\n",
       "0  51624    male   34     30-39      409.0  White   NaN   High School   \n",
       "1  51630  female   49     40-49      596.0  White   NaN  Some College   \n",
       "2  51638    male    9       0-9      115.0  White   NaN           NaN   \n",
       "3  51646    male    8       0-9      101.0  White   NaN           NaN   \n",
       "4  51647  female   45     40-49      541.0  White   NaN  College Grad   \n",
       "\n",
       "  MaritalStatus     HHIncome   ...     HardDrugs  SexEver  SexAge  \\\n",
       "0       Married  25000-34999   ...           Yes      Yes    16.0   \n",
       "1   LivePartner  35000-44999   ...           Yes      Yes    12.0   \n",
       "2           NaN  75000-99999   ...           NaN      NaN     NaN   \n",
       "3           NaN  55000-64999   ...           NaN      NaN     NaN   \n",
       "4       Married  75000-99999   ...            No      Yes    13.0   \n",
       "\n",
       "  SexNumPartnLife SexNumPartYear  SameSex  SexOrientation  PregnantNow  flu  \\\n",
       "0             8.0            1.0       No    Heterosexual          NaN    0   \n",
       "1            10.0            1.0      Yes    Heterosexual          NaN    0   \n",
       "2             NaN            NaN      NaN             NaN          NaN    0   \n",
       "3             NaN            NaN      NaN             NaN          NaN    0   \n",
       "4            20.0            0.0      Yes        Bisexual          NaN    0   \n",
       "\n",
       "   flutype  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"shape: \", df_flu.shape\n",
    "df_flu[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform column headers to lower case\n",
    "df_flu.columns = map(str.lower, df_flu.columns)\n",
    "df_flu_fin.columns = map(str.lower, df_flu_fin.columns)\n",
    "\n",
    "# Replace whitespace in headers\n",
    "df_flu.columns = [x.strip().replace(' ', '_') for x in df_flu.columns]\n",
    "df_flu_fin.columns = [x.strip().replace(' ', '_') for x in df_flu_fin.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable names and variable encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'gender' 'age' 'agedecade' 'agemonths' 'race1' 'race3' 'education'\n",
      " 'maritalstatus' 'hhincome' 'hhincomemid' 'poverty' 'homerooms' 'homeown'\n",
      " 'work' 'weight' 'length' 'headcirc' 'height' 'bmi' 'bmicatunder20yrs'\n",
      " 'bmi_who' 'pulse' 'bpsysave' 'bpdiaave' 'bpsys1' 'bpdia1' 'bpsys2'\n",
      " 'bpdia2' 'bpsys3' 'bpdia3' 'testosterone' 'directchol' 'totchol'\n",
      " 'urinevol1' 'urineflow1' 'urinevol2' 'urineflow2' 'diabetes' 'diabetesage'\n",
      " 'healthgen' 'daysmenthlthbad' 'littleinterest' 'depressed' 'npregnancies'\n",
      " 'nbabies' 'age1stbaby' 'sleephrsnight' 'sleeptrouble' 'physactive'\n",
      " 'physactivedays' 'tvhrsday' 'comphrsday' 'tvhrsdaychild' 'comphrsdaychild'\n",
      " 'alcohol12plusyr' 'alcoholday' 'alcoholyear' 'smokenow' 'smoke100'\n",
      " 'smoke100n' 'smokeage' 'marijuana' 'agefirstmarij' 'regularmarij'\n",
      " 'ageregmarij' 'harddrugs' 'sexever' 'sexage' 'sexnumpartnlife'\n",
      " 'sexnumpartyear' 'samesex' 'sexorientation' 'pregnantnow' 'flu' 'flutype']\n"
     ]
    }
   ],
   "source": [
    "# Extract variable names\n",
    "var_names = df_flu.columns.values\n",
    "print var_names\n",
    "\n",
    "# List of columns to be converted to floating point\n",
    "cat_var = df_flu.dtypes[df_flu.dtypes == \"object\"].index.values\n",
    "num_var = var_names[~df_flu.columns.isin(cat_var)]\n",
    "\n",
    "# Converted columns to floating point\n",
    "for feature_name in num_var:\n",
    "    df_flu.loc[:, feature_name] = df_flu.loc[:, feature_name].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract variable names\n",
    "var_names = df_flu_fin.columns.values\n",
    "\n",
    "# List of columns to be converted to floating point\n",
    "cat_var = df_flu_fin.dtypes[df_flu_fin.dtypes == \"object\"].index.values\n",
    "num_var = var_names[~df_flu_fin.columns.isin(cat_var)]\n",
    "\n",
    "for feature_name in num_var:\n",
    "    df_flu_fin.loc[:, feature_name] = df_flu_fin.loc[:, feature_name].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count missings\n",
    "df_null = table(df_flu[var_names[0]].isnull())\n",
    "for i in range(1, len(var_names)):\n",
    "    df_new = table(df_flu[var_names[i]].isnull())\n",
    "    df_null = pd.concat([df_null, df_new], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>agedecade</th>\n",
       "      <th>agemonths</th>\n",
       "      <th>race1</th>\n",
       "      <th>race3</th>\n",
       "      <th>education</th>\n",
       "      <th>maritalstatus</th>\n",
       "      <th>hhincome</th>\n",
       "      <th>hhincomemid</th>\n",
       "      <th>poverty</th>\n",
       "      <th>homerooms</th>\n",
       "      <th>homeown</th>\n",
       "      <th>work</th>\n",
       "      <th>weight</th>\n",
       "      <th>length</th>\n",
       "      <th>headcirc</th>\n",
       "      <th>height</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bmicatunder20yrs</th>\n",
       "      <th>bmi_who</th>\n",
       "      <th>pulse</th>\n",
       "      <th>bpsysave</th>\n",
       "      <th>bpdiaave</th>\n",
       "      <th>bpsys1</th>\n",
       "      <th>bpdia1</th>\n",
       "      <th>bpsys2</th>\n",
       "      <th>bpdia2</th>\n",
       "      <th>bpsys3</th>\n",
       "      <th>bpdia3</th>\n",
       "      <th>testosterone</th>\n",
       "      <th>directchol</th>\n",
       "      <th>totchol</th>\n",
       "      <th>urinevol1</th>\n",
       "      <th>urineflow1</th>\n",
       "      <th>urinevol2</th>\n",
       "      <th>urineflow2</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>diabetesage</th>\n",
       "      <th>healthgen</th>\n",
       "      <th>daysmenthlthbad</th>\n",
       "      <th>littleinterest</th>\n",
       "      <th>depressed</th>\n",
       "      <th>npregnancies</th>\n",
       "      <th>nbabies</th>\n",
       "      <th>age1stbaby</th>\n",
       "      <th>sleephrsnight</th>\n",
       "      <th>sleeptrouble</th>\n",
       "      <th>physactive</th>\n",
       "      <th>physactivedays</th>\n",
       "      <th>tvhrsday</th>\n",
       "      <th>comphrsday</th>\n",
       "      <th>tvhrsdaychild</th>\n",
       "      <th>comphrsdaychild</th>\n",
       "      <th>alcohol12plusyr</th>\n",
       "      <th>alcoholday</th>\n",
       "      <th>alcoholyear</th>\n",
       "      <th>smokenow</th>\n",
       "      <th>smoke100</th>\n",
       "      <th>smoke100n</th>\n",
       "      <th>smokeage</th>\n",
       "      <th>marijuana</th>\n",
       "      <th>agefirstmarij</th>\n",
       "      <th>regularmarij</th>\n",
       "      <th>ageregmarij</th>\n",
       "      <th>harddrugs</th>\n",
       "      <th>sexever</th>\n",
       "      <th>sexage</th>\n",
       "      <th>sexnumpartnlife</th>\n",
       "      <th>sexnumpartyear</th>\n",
       "      <th>samesex</th>\n",
       "      <th>sexorientation</th>\n",
       "      <th>pregnantnow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>5246.0</td>\n",
       "      <td>5246.0</td>\n",
       "      <td>5246.0</td>\n",
       "      <td>5057</td>\n",
       "      <td>2726</td>\n",
       "      <td>5246.0</td>\n",
       "      <td>2508</td>\n",
       "      <td>3574</td>\n",
       "      <td>3580</td>\n",
       "      <td>4798</td>\n",
       "      <td>4798</td>\n",
       "      <td>4843</td>\n",
       "      <td>5210</td>\n",
       "      <td>5213</td>\n",
       "      <td>3889</td>\n",
       "      <td>5206</td>\n",
       "      <td>356</td>\n",
       "      <td>61</td>\n",
       "      <td>5014</td>\n",
       "      <td>5010</td>\n",
       "      <td>724</td>\n",
       "      <td>4990</td>\n",
       "      <td>4376</td>\n",
       "      <td>4369</td>\n",
       "      <td>4369</td>\n",
       "      <td>4213</td>\n",
       "      <td>4213</td>\n",
       "      <td>4263</td>\n",
       "      <td>4263</td>\n",
       "      <td>4261</td>\n",
       "      <td>4261</td>\n",
       "      <td>2008</td>\n",
       "      <td>4337</td>\n",
       "      <td>4337</td>\n",
       "      <td>4637</td>\n",
       "      <td>4300</td>\n",
       "      <td>763</td>\n",
       "      <td>763</td>\n",
       "      <td>5143</td>\n",
       "      <td>330</td>\n",
       "      <td>3783</td>\n",
       "      <td>3778</td>\n",
       "      <td>3292</td>\n",
       "      <td>3295</td>\n",
       "      <td>1283</td>\n",
       "      <td>1182</td>\n",
       "      <td>941</td>\n",
       "      <td>3879</td>\n",
       "      <td>3890</td>\n",
       "      <td>4204</td>\n",
       "      <td>2374</td>\n",
       "      <td>2417</td>\n",
       "      <td>2419</td>\n",
       "      <td>417</td>\n",
       "      <td>417</td>\n",
       "      <td>3232</td>\n",
       "      <td>2404</td>\n",
       "      <td>2882</td>\n",
       "      <td>1539</td>\n",
       "      <td>3581</td>\n",
       "      <td>3581</td>\n",
       "      <td>1479</td>\n",
       "      <td>2411</td>\n",
       "      <td>1331</td>\n",
       "      <td>2411</td>\n",
       "      <td>629</td>\n",
       "      <td>2798</td>\n",
       "      <td>2796</td>\n",
       "      <td>2673</td>\n",
       "      <td>2773</td>\n",
       "      <td>2405</td>\n",
       "      <td>2798</td>\n",
       "      <td>2357</td>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>2520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2738</td>\n",
       "      <td>1672</td>\n",
       "      <td>1666</td>\n",
       "      <td>448</td>\n",
       "      <td>448</td>\n",
       "      <td>403</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>1357</td>\n",
       "      <td>40</td>\n",
       "      <td>4890</td>\n",
       "      <td>5185</td>\n",
       "      <td>232</td>\n",
       "      <td>236</td>\n",
       "      <td>4522</td>\n",
       "      <td>256</td>\n",
       "      <td>870</td>\n",
       "      <td>877</td>\n",
       "      <td>877</td>\n",
       "      <td>1033</td>\n",
       "      <td>1033</td>\n",
       "      <td>983</td>\n",
       "      <td>983</td>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>3238</td>\n",
       "      <td>909</td>\n",
       "      <td>909</td>\n",
       "      <td>609</td>\n",
       "      <td>946</td>\n",
       "      <td>4483</td>\n",
       "      <td>4483</td>\n",
       "      <td>103</td>\n",
       "      <td>4916</td>\n",
       "      <td>1463</td>\n",
       "      <td>1468</td>\n",
       "      <td>1954</td>\n",
       "      <td>1951</td>\n",
       "      <td>3963</td>\n",
       "      <td>4064</td>\n",
       "      <td>4305</td>\n",
       "      <td>1367</td>\n",
       "      <td>1356</td>\n",
       "      <td>1042</td>\n",
       "      <td>2872</td>\n",
       "      <td>2829</td>\n",
       "      <td>2827</td>\n",
       "      <td>4829</td>\n",
       "      <td>4829</td>\n",
       "      <td>2014</td>\n",
       "      <td>2842</td>\n",
       "      <td>2364</td>\n",
       "      <td>3707</td>\n",
       "      <td>1665</td>\n",
       "      <td>1665</td>\n",
       "      <td>3767</td>\n",
       "      <td>2835</td>\n",
       "      <td>3915</td>\n",
       "      <td>2835</td>\n",
       "      <td>4617</td>\n",
       "      <td>2448</td>\n",
       "      <td>2450</td>\n",
       "      <td>2573</td>\n",
       "      <td>2473</td>\n",
       "      <td>2841</td>\n",
       "      <td>2448</td>\n",
       "      <td>2889</td>\n",
       "      <td>4363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  gender     age  agedecade  agemonths   race1  race3  education  \\\n",
       "False  5246.0  5246.0  5246.0       5057       2726  5246.0   2508       3574   \n",
       "True      NaN     NaN     NaN        189       2520     NaN   2738       1672   \n",
       "\n",
       "       maritalstatus  hhincome  hhincomemid  poverty  homerooms  homeown  \\\n",
       "False           3580      4798         4798     4843       5210     5213   \n",
       "True            1666       448          448      403         36       33   \n",
       "\n",
       "       work  weight  length  headcirc  height   bmi  bmicatunder20yrs  \\\n",
       "False  3889    5206     356        61    5014  5010               724   \n",
       "True   1357      40    4890      5185     232   236              4522   \n",
       "\n",
       "       bmi_who  pulse  bpsysave  bpdiaave  bpsys1  bpdia1  bpsys2  bpdia2  \\\n",
       "False     4990   4376      4369      4369    4213    4213    4263    4263   \n",
       "True       256    870       877       877    1033    1033     983     983   \n",
       "\n",
       "       bpsys3  bpdia3  testosterone  directchol  totchol  urinevol1  \\\n",
       "False    4261    4261          2008        4337     4337       4637   \n",
       "True      985     985          3238         909      909        609   \n",
       "\n",
       "       urineflow1  urinevol2  urineflow2  diabetes  diabetesage  healthgen  \\\n",
       "False        4300        763         763      5143          330       3783   \n",
       "True          946       4483        4483       103         4916       1463   \n",
       "\n",
       "       daysmenthlthbad  littleinterest  depressed  npregnancies  nbabies  \\\n",
       "False             3778            3292       3295          1283     1182   \n",
       "True              1468            1954       1951          3963     4064   \n",
       "\n",
       "       age1stbaby  sleephrsnight  sleeptrouble  physactive  physactivedays  \\\n",
       "False         941           3879          3890        4204            2374   \n",
       "True         4305           1367          1356        1042            2872   \n",
       "\n",
       "       tvhrsday  comphrsday  tvhrsdaychild  comphrsdaychild  alcohol12plusyr  \\\n",
       "False      2417        2419            417              417             3232   \n",
       "True       2829        2827           4829             4829             2014   \n",
       "\n",
       "       alcoholday  alcoholyear  smokenow  smoke100  smoke100n  smokeage  \\\n",
       "False        2404         2882      1539      3581       3581      1479   \n",
       "True         2842         2364      3707      1665       1665      3767   \n",
       "\n",
       "       marijuana  agefirstmarij  regularmarij  ageregmarij  harddrugs  \\\n",
       "False       2411           1331          2411          629       2798   \n",
       "True        2835           3915          2835         4617       2448   \n",
       "\n",
       "       sexever  sexage  sexnumpartnlife  sexnumpartyear  samesex  \\\n",
       "False     2796    2673             2773            2405     2798   \n",
       "True      2450    2573             2473            2841     2448   \n",
       "\n",
       "       sexorientation  pregnantnow  \n",
       "False            2357          883  \n",
       "True             2889         4363  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_null.loc[:, ~df_null.iloc[1, :].isnull().values]\n",
    "df_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If missing and categorical fill the variable with 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df flu\n",
    "#-------\n",
    "# Transform categorical na's to unknown\n",
    "df_flu[cat_var] = df_flu[cat_var].fillna('Unknown')\n",
    "\n",
    "# Transform sensible numerical na's to zero\n",
    "df_flu.loc[df_flu.npregnancies.isnull(), 'npregnancies'] = 0\n",
    "df_flu.loc[df_flu.age1stbaby.isnull(), 'age1stbaby'] = 0\n",
    "df_flu.loc[df_flu.smokeage.isnull(), 'smokeage'] = 0\n",
    "\n",
    "# Replace homerooms with mean value\n",
    "df_flu.loc[df_flu.homerooms.isnull(), 'homerooms'] = round(df_flu.loc[:, 'homerooms'].mean(), 0)\n",
    "df_flu.loc[df_flu.daysmenthlthbad.isnull(), 'daysmenthlthbad'] = round(df_flu.loc[:, 'daysmenthlthbad'].mean(), 0)\n",
    "df_flu.loc[df_flu.homerooms.isnull(), 'hhincomemid'] = round(df_flu.loc[:, 'hhincomemid'].mean(), 0)\n",
    "df_flu.loc[df_flu.bmi.isnull(), 'bmi'] = round(df_flu.loc[:, 'bmi'].mean(), 0)\n",
    "df_flu.loc[df_flu.pulse.isnull(), 'pulse'] = round(df_flu.loc[:, 'pulse'].mean(), 0)\n",
    "df_flu.loc[df_flu.hhincomemid.isnull(), 'hhincomemid'] = round(df_flu.loc[:, 'hhincomemid'].mean(), 0)\n",
    "df_flu.loc[df_flu.directchol.isnull(), 'directchol'] = round(df_flu.loc[:, 'directchol'].mean(), 0)\n",
    "df_flu.loc[df_flu.totchol.isnull(), 'totchol'] = round(df_flu.loc[:, 'totchol'].mean(), 0)\n",
    "df_flu.loc[df_flu.sleephrsnight.isnull(), 'sleephrsnight'] = round(df_flu.loc[:, 'sleephrsnight'].mean(), 0)\n",
    "df_flu.loc[df_flu.physactivedays.isnull(), 'physactivedays'] = round(df_flu.loc[:, 'physactivedays'].mean(), 0)\n",
    "df_flu.loc[df_flu.alcoholday.isnull(), 'alcoholday'] = round(df_flu.loc[:, 'alcoholday'].mean(), 0)\n",
    "df_flu.loc[df_flu.alcoholyear.isnull(), 'alcoholyear'] = round(df_flu.loc[:, 'alcoholyear'].mean(), 0)\n",
    "\n",
    "# df flu fin\n",
    "#-----------\n",
    "# Transform categorical na's to unknown\n",
    "df_flu_fin[cat_var] = df_flu_fin[cat_var].fillna('Unknown')\n",
    "\n",
    "# Transform sensible numerical na's to zero\n",
    "df_flu_fin.loc[df_flu_fin.npregnancies.isnull(), 'npregnancies'] = 0\n",
    "df_flu_fin.loc[df_flu_fin.age1stbaby.isnull(), 'age1stbaby'] = 0\n",
    "df_flu_fin.loc[df_flu_fin.smokeage.isnull(), 'smokeage'] = 0\n",
    "\n",
    "# Replace homerooms with mean value\n",
    "df_flu_fin.loc[df_flu_fin.homerooms.isnull(), 'homerooms'] = round(df_flu_fin.loc[:, 'homerooms'].mean(), 0)\n",
    "df_flu_fin.loc[df_flu_fin.daysmenthlthbad.isnull(), 'daysmenthlthbad'] = round(df_flu_fin.loc[:, 'daysmenthlthbad'].mean(), 0)\n",
    "df_flu_fin.loc[df_flu_fin.homerooms.isnull(), 'hhincomemid'] = round(df_flu_fin.loc[:, 'hhincomemid'].mean(), 0)\n",
    "df_flu_fin.loc[df_flu_fin.bmi.isnull(), 'bmi'] = round(df_flu_fin.loc[:, 'bmi'].mean(), 0)\n",
    "df_flu_fin.loc[df_flu_fin.pulse.isnull(), 'pulse'] = round(df_flu_fin.loc[:, 'pulse'].mean(), 0)\n",
    "df_flu_fin.loc[df_flu_fin.hhincomemid.isnull(), 'hhincomemid'] = round(df_flu_fin.loc[:, 'hhincomemid'].mean(), 0)\n",
    "df_flu_fin.loc[df_flu_fin.directchol.isnull(), 'directchol'] = round(df_flu_fin.loc[:, 'directchol'].mean(), 0)\n",
    "df_flu_fin.loc[df_flu_fin.totchol.isnull(), 'totchol'] = round(df_flu_fin.loc[:, 'totchol'].mean(), 0)\n",
    "df_flu_fin.loc[df_flu_fin.sleephrsnight.isnull(), 'sleephrsnight'] = round(df_flu_fin.loc[:, 'sleephrsnight'].mean(), 0)\n",
    "df_flu_fin.loc[df_flu_fin.physactivedays.isnull(), 'physactivedays'] = round(df_flu_fin.loc[:, 'physactivedays'].mean(), 0)\n",
    "df_flu_fin.loc[df_flu_fin.alcoholday.isnull(), 'alcoholday'] = round(df_flu_fin.loc[:, 'alcoholday'].mean(), 0)\n",
    "df_flu_fin.loc[df_flu_fin.alcoholyear.isnull(), 'alcoholyear'] = round(df_flu_fin.loc[:, 'alcoholyear'].mean(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** id **\n",
    "The id variable is the unique identifier. It has to be excluded.\n",
    "\n",
    "** Age **\n",
    "As can be seen above, agedecade and agemonts have missing values while the age has non. Age in months is completly excluded as is has no many missings and is probalby not an important predictor anyhow. The age in decades can be derived from age and is heavily corellated as well.\n",
    "\n",
    "** Race **\n",
    "The race variable 1 encodes the same races as race 3. However race 3 has additionaly the category Asian but has missings. The variables are combined to a new variable where there are no missings.\n",
    "\n",
    "** Length, headcirc, bmicatunder20yrs, diabetesage, tvhrsdaychild, comphrsdaychild, ageregmarij, testosterone, agefirstmarij **\n",
    "They have so many missings that they are very difficult to use and other variables either control for them or it is reasanable to exclude them. \n",
    "\n",
    "** Ethical note:** In this context it is ethiticly sound to use race and gender as variables. This is because they could have explenatory power but identify correctly the flu has no discriminatory effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df flu\n",
    "#-------\n",
    "# Education\n",
    "df_flu.loc[df_flu.education.isnull(), 'education'] = \"Unknown\"\n",
    "\n",
    "# Recode martial status\n",
    "df_flu.loc[~((df_flu.maritalstatus.isnull()) | (df_flu.maritalstatus == 'Married')), 'maritalstatus'] = \"Not_Married\"\n",
    "df_flu.loc[df_flu.maritalstatus.isnull(), 'maritalstatus'] = \"Unknown\"\n",
    "\n",
    "# Replace race values \n",
    "df_flu.loc[:, 'race'] = df_flu['race3']\n",
    "df_flu.loc[df_flu['race'].isnull(), 'race'] = df_flu.loc[df_flu['race'].isnull(), 'race1']\n",
    "\n",
    "# Drop variables \n",
    "drop_var = [\"id\", \"agedecade\", \"agemonths\", \"race1\", \"race3\", \"length\", \"headcirc\", \"testosterone\", \"bmicatunder20yrs\", \"diabetesage\", \"tvhrsdaychild\", \"comphrsdaychild\", \"ageregmarij\", \"agefirstmarij\"]\n",
    "keep_var = df_flu.columns[~df_flu.columns.isin(drop_var)]\n",
    "df_flu = df_flu[keep_var]\n",
    "\n",
    "# df flu fin\n",
    "#-----------\n",
    "# Education\n",
    "df_flu_fin.loc[df_flu_fin.education.isnull(), 'education'] = \"Unknown\"\n",
    "\n",
    "# Recode martial status\n",
    "df_flu_fin.loc[~((df_flu_fin.maritalstatus.isnull()) | (df_flu_fin.maritalstatus == 'Married')), 'maritalstatus'] = \"Not_Married\"\n",
    "df_flu_fin.loc[df_flu_fin.maritalstatus.isnull(), 'maritalstatus'] = \"Unknown\"\n",
    "\n",
    "# Replace race values \n",
    "df_flu_fin.loc[:, 'race'] = df_flu_fin['race3']\n",
    "df_flu_fin.loc[df_flu['race'].isnull(), 'race'] = df_flu_fin.loc[df_flu_fin['race'].isnull(), 'race1']\n",
    "\n",
    "# Drop variables \n",
    "drop_var = [\"id\", \"agedecade\", \"agemonths\", \"race1\", \"race3\", \"length\", \"headcirc\", \"testosterone\", \"bmicatunder20yrs\", \"diabetesage\", \"tvhrsdaychild\", \"comphrsdaychild\", \"ageregmarij\", \"agefirstmarij\"]\n",
    "keep_var = df_flu_fin.columns[~df_flu_fin.columns.isin(drop_var)]\n",
    "df_flu_fin = df_flu_fin[keep_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories with only a few occurunces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df flu\n",
    "#-------\n",
    "df_flu.loc[df_flu.homeown == 'Unknown', 'homeown'] = \"Other\"\n",
    "df_flu.loc[df_flu.work == 'Unknown', 'work'] = \"Other\"\n",
    "df_flu.loc[df_flu.tvhrsday.isin([\"3_hr\", \"4_hr\", \"More_4_hr\"]), 'tvhrsday'] = \"3_or_more_hr\"\n",
    "\n",
    "# df flu fin\n",
    "#-----------\n",
    "df_flu_fin.loc[df_flu_fin.homeown == 'Unknown', 'homeown'] = \"Other\"\n",
    "df_flu_fin.loc[df_flu_fin.work == 'Unknown', 'work'] = \"Other\"\n",
    "df_flu_fin.loc[df_flu_fin.tvhrsday.isin([\"3_hr\", \"4_hr\", \"More_4_hr\"]), 'tvhrsday'] = \"3_or_more_hr\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spearman rank correlation coefficient, rs , is a nonparametric measure of correlation based on data ranks. It is obtained by ranking the values of the two variables (X and Y) and calculating the Pearson rp on the resulting ranks. The advantage of the Spearman rank correlation coefficient is that the X and Y values can be continuous or ordinal. That is why it is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAASHCAYAAAB1b23cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9kiR8MH51qwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc884940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correlation \n",
    "corr_matrix = df_flu.corr(method=\"spearman\")\n",
    "\n",
    "# Display heat map \n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n",
    "\n",
    "ax.pcolor(corr_matrix)\n",
    "ax.set_title('Heatmap of correlation matrix')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heat map as well as the correlation matrix shows that some of the predictors (marked by the color red) are correlated amongst themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>hhincomemid</th>\n",
       "      <th>poverty</th>\n",
       "      <th>homerooms</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pulse</th>\n",
       "      <th>bpsysave</th>\n",
       "      <th>bpdiaave</th>\n",
       "      <th>bpsys1</th>\n",
       "      <th>bpdia1</th>\n",
       "      <th>bpsys2</th>\n",
       "      <th>bpdia2</th>\n",
       "      <th>bpsys3</th>\n",
       "      <th>bpdia3</th>\n",
       "      <th>directchol</th>\n",
       "      <th>totchol</th>\n",
       "      <th>urinevol1</th>\n",
       "      <th>urineflow1</th>\n",
       "      <th>urinevol2</th>\n",
       "      <th>urineflow2</th>\n",
       "      <th>daysmenthlthbad</th>\n",
       "      <th>npregnancies</th>\n",
       "      <th>nbabies</th>\n",
       "      <th>age1stbaby</th>\n",
       "      <th>sleephrsnight</th>\n",
       "      <th>physactivedays</th>\n",
       "      <th>alcoholday</th>\n",
       "      <th>alcoholyear</th>\n",
       "      <th>smokeage</th>\n",
       "      <th>sexage</th>\n",
       "      <th>sexnumpartnlife</th>\n",
       "      <th>sexnumpartyear</th>\n",
       "      <th>flu</th>\n",
       "      <th>flutype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028933</td>\n",
       "      <td>0.193999</td>\n",
       "      <td>0.034219</td>\n",
       "      <td>0.557354</td>\n",
       "      <td>0.382616</td>\n",
       "      <td>0.472411</td>\n",
       "      <td>-0.223376</td>\n",
       "      <td>0.546726</td>\n",
       "      <td>0.292232</td>\n",
       "      <td>0.561878</td>\n",
       "      <td>0.325356</td>\n",
       "      <td>0.540107</td>\n",
       "      <td>0.295472</td>\n",
       "      <td>0.533722</td>\n",
       "      <td>0.276902</td>\n",
       "      <td>0.229572</td>\n",
       "      <td>0.252555</td>\n",
       "      <td>-0.025893</td>\n",
       "      <td>0.097017</td>\n",
       "      <td>0.015558</td>\n",
       "      <td>-0.089875</td>\n",
       "      <td>-0.291180</td>\n",
       "      <td>0.413983</td>\n",
       "      <td>0.301199</td>\n",
       "      <td>0.383298</td>\n",
       "      <td>-0.014003</td>\n",
       "      <td>-0.010234</td>\n",
       "      <td>-0.318159</td>\n",
       "      <td>-0.219312</td>\n",
       "      <td>0.399126</td>\n",
       "      <td>0.225003</td>\n",
       "      <td>0.074638</td>\n",
       "      <td>-0.199486</td>\n",
       "      <td>0.086335</td>\n",
       "      <td>0.087716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hhincomemid</th>\n",
       "      <td>0.028933</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.894293</td>\n",
       "      <td>0.452291</td>\n",
       "      <td>0.026768</td>\n",
       "      <td>0.127238</td>\n",
       "      <td>-0.048542</td>\n",
       "      <td>-0.068457</td>\n",
       "      <td>-0.066320</td>\n",
       "      <td>0.044845</td>\n",
       "      <td>-0.069352</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>-0.064774</td>\n",
       "      <td>0.043117</td>\n",
       "      <td>-0.063799</td>\n",
       "      <td>0.054107</td>\n",
       "      <td>0.095501</td>\n",
       "      <td>0.025612</td>\n",
       "      <td>0.049177</td>\n",
       "      <td>0.102159</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>0.037826</td>\n",
       "      <td>-0.099976</td>\n",
       "      <td>-0.042527</td>\n",
       "      <td>-0.211471</td>\n",
       "      <td>-0.026522</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>-0.021698</td>\n",
       "      <td>-0.152383</td>\n",
       "      <td>0.090485</td>\n",
       "      <td>-0.056427</td>\n",
       "      <td>0.177402</td>\n",
       "      <td>-0.054467</td>\n",
       "      <td>-0.036310</td>\n",
       "      <td>-0.078384</td>\n",
       "      <td>-0.078572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poverty</th>\n",
       "      <td>0.193999</td>\n",
       "      <td>0.894293</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.408328</td>\n",
       "      <td>0.104983</td>\n",
       "      <td>0.182484</td>\n",
       "      <td>0.022712</td>\n",
       "      <td>-0.106283</td>\n",
       "      <td>0.022357</td>\n",
       "      <td>0.082640</td>\n",
       "      <td>0.023606</td>\n",
       "      <td>0.081067</td>\n",
       "      <td>0.024528</td>\n",
       "      <td>0.079009</td>\n",
       "      <td>0.020303</td>\n",
       "      <td>0.088571</td>\n",
       "      <td>0.123877</td>\n",
       "      <td>0.064914</td>\n",
       "      <td>0.029671</td>\n",
       "      <td>0.112139</td>\n",
       "      <td>0.025737</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>-0.149445</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>-0.210000</td>\n",
       "      <td>0.026015</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>-0.000803</td>\n",
       "      <td>-0.234888</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.217386</td>\n",
       "      <td>-0.025297</td>\n",
       "      <td>-0.051002</td>\n",
       "      <td>-0.055360</td>\n",
       "      <td>-0.055144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homerooms</th>\n",
       "      <td>0.034219</td>\n",
       "      <td>0.452291</td>\n",
       "      <td>0.408328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013075</td>\n",
       "      <td>0.028887</td>\n",
       "      <td>-0.062440</td>\n",
       "      <td>-0.020503</td>\n",
       "      <td>-0.026229</td>\n",
       "      <td>-0.037116</td>\n",
       "      <td>-0.024313</td>\n",
       "      <td>-0.030235</td>\n",
       "      <td>-0.026956</td>\n",
       "      <td>-0.037979</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.027618</td>\n",
       "      <td>0.076992</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>-0.021917</td>\n",
       "      <td>0.016319</td>\n",
       "      <td>0.037742</td>\n",
       "      <td>0.046641</td>\n",
       "      <td>-0.057821</td>\n",
       "      <td>0.025860</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>0.065734</td>\n",
       "      <td>0.040207</td>\n",
       "      <td>-0.005430</td>\n",
       "      <td>-0.102930</td>\n",
       "      <td>0.068513</td>\n",
       "      <td>-0.047281</td>\n",
       "      <td>0.104643</td>\n",
       "      <td>-0.047289</td>\n",
       "      <td>-0.015147</td>\n",
       "      <td>-0.056579</td>\n",
       "      <td>-0.056848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.557354</td>\n",
       "      <td>0.026768</td>\n",
       "      <td>0.104983</td>\n",
       "      <td>-0.013075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.687848</td>\n",
       "      <td>0.856757</td>\n",
       "      <td>-0.107407</td>\n",
       "      <td>0.355787</td>\n",
       "      <td>0.335541</td>\n",
       "      <td>0.355865</td>\n",
       "      <td>0.342757</td>\n",
       "      <td>0.348634</td>\n",
       "      <td>0.336996</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.312529</td>\n",
       "      <td>-0.062876</td>\n",
       "      <td>0.102681</td>\n",
       "      <td>0.115205</td>\n",
       "      <td>0.136562</td>\n",
       "      <td>0.020222</td>\n",
       "      <td>-0.030979</td>\n",
       "      <td>-0.241684</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.025257</td>\n",
       "      <td>0.093522</td>\n",
       "      <td>-0.049063</td>\n",
       "      <td>-0.055953</td>\n",
       "      <td>-0.093658</td>\n",
       "      <td>-0.220016</td>\n",
       "      <td>0.286683</td>\n",
       "      <td>-0.045716</td>\n",
       "      <td>0.101659</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>0.035597</td>\n",
       "      <td>0.036042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>0.382616</td>\n",
       "      <td>0.127238</td>\n",
       "      <td>0.182484</td>\n",
       "      <td>0.028887</td>\n",
       "      <td>0.687848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384586</td>\n",
       "      <td>-0.180858</td>\n",
       "      <td>0.223047</td>\n",
       "      <td>0.262312</td>\n",
       "      <td>0.227286</td>\n",
       "      <td>0.262777</td>\n",
       "      <td>0.213104</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>0.226284</td>\n",
       "      <td>0.254311</td>\n",
       "      <td>-0.008445</td>\n",
       "      <td>0.021482</td>\n",
       "      <td>0.178908</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>0.072229</td>\n",
       "      <td>0.018079</td>\n",
       "      <td>-0.235660</td>\n",
       "      <td>-0.171957</td>\n",
       "      <td>-0.143220</td>\n",
       "      <td>-0.149076</td>\n",
       "      <td>-0.030406</td>\n",
       "      <td>-0.047966</td>\n",
       "      <td>-0.052635</td>\n",
       "      <td>-0.044471</td>\n",
       "      <td>0.273081</td>\n",
       "      <td>-0.050732</td>\n",
       "      <td>0.212346</td>\n",
       "      <td>0.059533</td>\n",
       "      <td>-0.016885</td>\n",
       "      <td>-0.016921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>0.472411</td>\n",
       "      <td>-0.048542</td>\n",
       "      <td>0.022712</td>\n",
       "      <td>-0.062440</td>\n",
       "      <td>0.856757</td>\n",
       "      <td>0.384586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055185</td>\n",
       "      <td>0.336773</td>\n",
       "      <td>0.280516</td>\n",
       "      <td>0.334985</td>\n",
       "      <td>0.293164</td>\n",
       "      <td>0.331414</td>\n",
       "      <td>0.282617</td>\n",
       "      <td>0.327302</td>\n",
       "      <td>0.257359</td>\n",
       "      <td>-0.142773</td>\n",
       "      <td>0.165600</td>\n",
       "      <td>0.050688</td>\n",
       "      <td>0.078677</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>-0.037196</td>\n",
       "      <td>-0.170142</td>\n",
       "      <td>0.231025</td>\n",
       "      <td>0.072117</td>\n",
       "      <td>0.191039</td>\n",
       "      <td>-0.042921</td>\n",
       "      <td>-0.048707</td>\n",
       "      <td>-0.087767</td>\n",
       "      <td>-0.247641</td>\n",
       "      <td>0.205083</td>\n",
       "      <td>-0.025283</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>-0.080398</td>\n",
       "      <td>0.061170</td>\n",
       "      <td>0.061793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulse</th>\n",
       "      <td>-0.223376</td>\n",
       "      <td>-0.068457</td>\n",
       "      <td>-0.106283</td>\n",
       "      <td>-0.020503</td>\n",
       "      <td>-0.107407</td>\n",
       "      <td>-0.180858</td>\n",
       "      <td>-0.055185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.124058</td>\n",
       "      <td>0.012708</td>\n",
       "      <td>-0.131726</td>\n",
       "      <td>0.011626</td>\n",
       "      <td>-0.121916</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>-0.116744</td>\n",
       "      <td>0.013131</td>\n",
       "      <td>-0.048940</td>\n",
       "      <td>-0.017322</td>\n",
       "      <td>-0.064175</td>\n",
       "      <td>-0.094796</td>\n",
       "      <td>-0.070399</td>\n",
       "      <td>-0.027971</td>\n",
       "      <td>0.130196</td>\n",
       "      <td>-0.019199</td>\n",
       "      <td>-0.087352</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>0.103364</td>\n",
       "      <td>0.019636</td>\n",
       "      <td>-0.060835</td>\n",
       "      <td>-0.005813</td>\n",
       "      <td>-0.054960</td>\n",
       "      <td>-0.046973</td>\n",
       "      <td>0.019463</td>\n",
       "      <td>0.019251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpsysave</th>\n",
       "      <td>0.546726</td>\n",
       "      <td>-0.066320</td>\n",
       "      <td>0.022357</td>\n",
       "      <td>-0.026229</td>\n",
       "      <td>0.355787</td>\n",
       "      <td>0.223047</td>\n",
       "      <td>0.336773</td>\n",
       "      <td>-0.124058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460086</td>\n",
       "      <td>0.941664</td>\n",
       "      <td>0.486047</td>\n",
       "      <td>0.985708</td>\n",
       "      <td>0.461620</td>\n",
       "      <td>0.985168</td>\n",
       "      <td>0.440902</td>\n",
       "      <td>-0.043118</td>\n",
       "      <td>0.231903</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.099369</td>\n",
       "      <td>0.025048</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>-0.135656</td>\n",
       "      <td>0.112632</td>\n",
       "      <td>0.191169</td>\n",
       "      <td>0.126684</td>\n",
       "      <td>-0.023148</td>\n",
       "      <td>-0.017517</td>\n",
       "      <td>-0.058497</td>\n",
       "      <td>-0.053425</td>\n",
       "      <td>0.208547</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>-0.078827</td>\n",
       "      <td>0.079941</td>\n",
       "      <td>0.081032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpdiaave</th>\n",
       "      <td>0.292232</td>\n",
       "      <td>0.044845</td>\n",
       "      <td>0.082640</td>\n",
       "      <td>-0.037116</td>\n",
       "      <td>0.335541</td>\n",
       "      <td>0.262312</td>\n",
       "      <td>0.280516</td>\n",
       "      <td>0.012708</td>\n",
       "      <td>0.460086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444820</td>\n",
       "      <td>0.904659</td>\n",
       "      <td>0.451284</td>\n",
       "      <td>0.975596</td>\n",
       "      <td>0.452844</td>\n",
       "      <td>0.976032</td>\n",
       "      <td>-0.050705</td>\n",
       "      <td>0.263140</td>\n",
       "      <td>0.087436</td>\n",
       "      <td>0.130654</td>\n",
       "      <td>0.067772</td>\n",
       "      <td>0.035348</td>\n",
       "      <td>-0.069105</td>\n",
       "      <td>0.058770</td>\n",
       "      <td>-0.025979</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>-0.064822</td>\n",
       "      <td>-0.034609</td>\n",
       "      <td>-0.081156</td>\n",
       "      <td>-0.035280</td>\n",
       "      <td>0.136479</td>\n",
       "      <td>0.029631</td>\n",
       "      <td>0.098724</td>\n",
       "      <td>-0.049160</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.003202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpsys1</th>\n",
       "      <td>0.561878</td>\n",
       "      <td>-0.069352</td>\n",
       "      <td>0.023606</td>\n",
       "      <td>-0.024313</td>\n",
       "      <td>0.355865</td>\n",
       "      <td>0.227286</td>\n",
       "      <td>0.334985</td>\n",
       "      <td>-0.131726</td>\n",
       "      <td>0.941664</td>\n",
       "      <td>0.444820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.470708</td>\n",
       "      <td>0.935021</td>\n",
       "      <td>0.447532</td>\n",
       "      <td>0.921360</td>\n",
       "      <td>0.423749</td>\n",
       "      <td>-0.034462</td>\n",
       "      <td>0.229614</td>\n",
       "      <td>0.028084</td>\n",
       "      <td>0.099326</td>\n",
       "      <td>0.027884</td>\n",
       "      <td>0.007310</td>\n",
       "      <td>-0.134170</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.203071</td>\n",
       "      <td>0.131523</td>\n",
       "      <td>-0.027819</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>-0.069076</td>\n",
       "      <td>-0.054089</td>\n",
       "      <td>0.212153</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.066088</td>\n",
       "      <td>-0.083511</td>\n",
       "      <td>0.079960</td>\n",
       "      <td>0.081090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpdia1</th>\n",
       "      <td>0.325356</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>0.081067</td>\n",
       "      <td>-0.030235</td>\n",
       "      <td>0.342757</td>\n",
       "      <td>0.262777</td>\n",
       "      <td>0.293164</td>\n",
       "      <td>0.011626</td>\n",
       "      <td>0.486047</td>\n",
       "      <td>0.904659</td>\n",
       "      <td>0.470708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.480138</td>\n",
       "      <td>0.890017</td>\n",
       "      <td>0.472029</td>\n",
       "      <td>0.877586</td>\n",
       "      <td>-0.043100</td>\n",
       "      <td>0.272627</td>\n",
       "      <td>0.070533</td>\n",
       "      <td>0.125160</td>\n",
       "      <td>0.048903</td>\n",
       "      <td>0.024347</td>\n",
       "      <td>-0.080022</td>\n",
       "      <td>0.078282</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.081537</td>\n",
       "      <td>-0.066430</td>\n",
       "      <td>-0.034676</td>\n",
       "      <td>-0.085774</td>\n",
       "      <td>-0.038428</td>\n",
       "      <td>0.144642</td>\n",
       "      <td>0.042818</td>\n",
       "      <td>0.114244</td>\n",
       "      <td>-0.059955</td>\n",
       "      <td>0.010591</td>\n",
       "      <td>0.010489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpsys2</th>\n",
       "      <td>0.540107</td>\n",
       "      <td>-0.064774</td>\n",
       "      <td>0.024528</td>\n",
       "      <td>-0.026956</td>\n",
       "      <td>0.348634</td>\n",
       "      <td>0.213104</td>\n",
       "      <td>0.331414</td>\n",
       "      <td>-0.121916</td>\n",
       "      <td>0.985708</td>\n",
       "      <td>0.451284</td>\n",
       "      <td>0.935021</td>\n",
       "      <td>0.480138</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455221</td>\n",
       "      <td>0.941570</td>\n",
       "      <td>0.432317</td>\n",
       "      <td>-0.039475</td>\n",
       "      <td>0.229129</td>\n",
       "      <td>0.029282</td>\n",
       "      <td>0.105439</td>\n",
       "      <td>0.013975</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.132745</td>\n",
       "      <td>0.108997</td>\n",
       "      <td>0.191697</td>\n",
       "      <td>0.122912</td>\n",
       "      <td>-0.024846</td>\n",
       "      <td>-0.017795</td>\n",
       "      <td>-0.058149</td>\n",
       "      <td>-0.049197</td>\n",
       "      <td>0.203273</td>\n",
       "      <td>0.013093</td>\n",
       "      <td>0.059108</td>\n",
       "      <td>-0.079389</td>\n",
       "      <td>0.079895</td>\n",
       "      <td>0.080962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpdia2</th>\n",
       "      <td>0.295472</td>\n",
       "      <td>0.043117</td>\n",
       "      <td>0.079009</td>\n",
       "      <td>-0.037979</td>\n",
       "      <td>0.336996</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>0.282617</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>0.461620</td>\n",
       "      <td>0.975596</td>\n",
       "      <td>0.447532</td>\n",
       "      <td>0.890017</td>\n",
       "      <td>0.455221</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.454356</td>\n",
       "      <td>0.905536</td>\n",
       "      <td>-0.048538</td>\n",
       "      <td>0.264022</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.119889</td>\n",
       "      <td>0.069348</td>\n",
       "      <td>0.041017</td>\n",
       "      <td>-0.071693</td>\n",
       "      <td>0.054395</td>\n",
       "      <td>-0.024730</td>\n",
       "      <td>0.055383</td>\n",
       "      <td>-0.068693</td>\n",
       "      <td>-0.034842</td>\n",
       "      <td>-0.068278</td>\n",
       "      <td>-0.030786</td>\n",
       "      <td>0.134411</td>\n",
       "      <td>0.032556</td>\n",
       "      <td>0.100783</td>\n",
       "      <td>-0.054891</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>0.008319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpsys3</th>\n",
       "      <td>0.533722</td>\n",
       "      <td>-0.063799</td>\n",
       "      <td>0.020303</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.226284</td>\n",
       "      <td>0.327302</td>\n",
       "      <td>-0.116744</td>\n",
       "      <td>0.985168</td>\n",
       "      <td>0.452844</td>\n",
       "      <td>0.921360</td>\n",
       "      <td>0.472029</td>\n",
       "      <td>0.941570</td>\n",
       "      <td>0.454356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.436190</td>\n",
       "      <td>-0.043418</td>\n",
       "      <td>0.228684</td>\n",
       "      <td>0.022178</td>\n",
       "      <td>0.092663</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>-0.011457</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>0.102536</td>\n",
       "      <td>0.178447</td>\n",
       "      <td>0.116311</td>\n",
       "      <td>-0.015968</td>\n",
       "      <td>-0.012373</td>\n",
       "      <td>-0.056619</td>\n",
       "      <td>-0.052894</td>\n",
       "      <td>0.205014</td>\n",
       "      <td>0.005861</td>\n",
       "      <td>0.059407</td>\n",
       "      <td>-0.073197</td>\n",
       "      <td>0.075550</td>\n",
       "      <td>0.076786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpdia3</th>\n",
       "      <td>0.276902</td>\n",
       "      <td>0.054107</td>\n",
       "      <td>0.088571</td>\n",
       "      <td>-0.027618</td>\n",
       "      <td>0.312529</td>\n",
       "      <td>0.254311</td>\n",
       "      <td>0.257359</td>\n",
       "      <td>0.013131</td>\n",
       "      <td>0.440902</td>\n",
       "      <td>0.976032</td>\n",
       "      <td>0.423749</td>\n",
       "      <td>0.877586</td>\n",
       "      <td>0.432317</td>\n",
       "      <td>0.905536</td>\n",
       "      <td>0.436190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.043060</td>\n",
       "      <td>0.252889</td>\n",
       "      <td>0.088967</td>\n",
       "      <td>0.133571</td>\n",
       "      <td>0.070360</td>\n",
       "      <td>0.044166</td>\n",
       "      <td>-0.062424</td>\n",
       "      <td>0.052355</td>\n",
       "      <td>-0.019899</td>\n",
       "      <td>0.055465</td>\n",
       "      <td>-0.058951</td>\n",
       "      <td>-0.032469</td>\n",
       "      <td>-0.089571</td>\n",
       "      <td>-0.031876</td>\n",
       "      <td>0.132276</td>\n",
       "      <td>0.026423</td>\n",
       "      <td>0.100719</td>\n",
       "      <td>-0.041058</td>\n",
       "      <td>-0.005755</td>\n",
       "      <td>-0.005528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directchol</th>\n",
       "      <td>0.229572</td>\n",
       "      <td>0.095501</td>\n",
       "      <td>0.123877</td>\n",
       "      <td>0.076992</td>\n",
       "      <td>-0.062876</td>\n",
       "      <td>-0.008445</td>\n",
       "      <td>-0.142773</td>\n",
       "      <td>-0.048940</td>\n",
       "      <td>-0.043118</td>\n",
       "      <td>-0.050705</td>\n",
       "      <td>-0.034462</td>\n",
       "      <td>-0.043100</td>\n",
       "      <td>-0.039475</td>\n",
       "      <td>-0.048538</td>\n",
       "      <td>-0.043418</td>\n",
       "      <td>-0.043060</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079563</td>\n",
       "      <td>-0.041384</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>0.036677</td>\n",
       "      <td>0.007343</td>\n",
       "      <td>-0.072483</td>\n",
       "      <td>0.242473</td>\n",
       "      <td>-0.062079</td>\n",
       "      <td>0.201397</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.017775</td>\n",
       "      <td>-0.138061</td>\n",
       "      <td>0.083916</td>\n",
       "      <td>0.044436</td>\n",
       "      <td>0.058096</td>\n",
       "      <td>-0.049657</td>\n",
       "      <td>-0.009608</td>\n",
       "      <td>-0.045022</td>\n",
       "      <td>-0.044682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totchol</th>\n",
       "      <td>0.252555</td>\n",
       "      <td>0.025612</td>\n",
       "      <td>0.064914</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.102681</td>\n",
       "      <td>0.021482</td>\n",
       "      <td>0.165600</td>\n",
       "      <td>-0.017322</td>\n",
       "      <td>0.231903</td>\n",
       "      <td>0.263140</td>\n",
       "      <td>0.229614</td>\n",
       "      <td>0.272627</td>\n",
       "      <td>0.229129</td>\n",
       "      <td>0.264022</td>\n",
       "      <td>0.228684</td>\n",
       "      <td>0.252889</td>\n",
       "      <td>0.079563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>0.070741</td>\n",
       "      <td>-0.001207</td>\n",
       "      <td>-0.054392</td>\n",
       "      <td>-0.031068</td>\n",
       "      <td>0.193315</td>\n",
       "      <td>0.063069</td>\n",
       "      <td>0.169215</td>\n",
       "      <td>-0.010038</td>\n",
       "      <td>-0.014338</td>\n",
       "      <td>-0.089703</td>\n",
       "      <td>-0.029324</td>\n",
       "      <td>0.101754</td>\n",
       "      <td>0.085658</td>\n",
       "      <td>0.026115</td>\n",
       "      <td>-0.124147</td>\n",
       "      <td>0.028720</td>\n",
       "      <td>0.029138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urinevol1</th>\n",
       "      <td>-0.025893</td>\n",
       "      <td>0.049177</td>\n",
       "      <td>0.029671</td>\n",
       "      <td>-0.021917</td>\n",
       "      <td>0.115205</td>\n",
       "      <td>0.178908</td>\n",
       "      <td>0.050688</td>\n",
       "      <td>-0.064175</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.087436</td>\n",
       "      <td>0.028084</td>\n",
       "      <td>0.070533</td>\n",
       "      <td>0.029282</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.022178</td>\n",
       "      <td>0.088967</td>\n",
       "      <td>-0.041384</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.713369</td>\n",
       "      <td>0.144779</td>\n",
       "      <td>0.134566</td>\n",
       "      <td>-0.047389</td>\n",
       "      <td>-0.104399</td>\n",
       "      <td>-0.078111</td>\n",
       "      <td>-0.097208</td>\n",
       "      <td>-0.036950</td>\n",
       "      <td>-0.028334</td>\n",
       "      <td>0.028795</td>\n",
       "      <td>0.007543</td>\n",
       "      <td>0.055175</td>\n",
       "      <td>-0.023486</td>\n",
       "      <td>0.053858</td>\n",
       "      <td>0.033827</td>\n",
       "      <td>-0.012236</td>\n",
       "      <td>-0.013221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urineflow1</th>\n",
       "      <td>0.097017</td>\n",
       "      <td>0.102159</td>\n",
       "      <td>0.112139</td>\n",
       "      <td>0.016319</td>\n",
       "      <td>0.136562</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>0.078677</td>\n",
       "      <td>-0.094796</td>\n",
       "      <td>0.099369</td>\n",
       "      <td>0.130654</td>\n",
       "      <td>0.099326</td>\n",
       "      <td>0.125160</td>\n",
       "      <td>0.105439</td>\n",
       "      <td>0.119889</td>\n",
       "      <td>0.092663</td>\n",
       "      <td>0.133571</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>0.070741</td>\n",
       "      <td>0.713369</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169825</td>\n",
       "      <td>0.163706</td>\n",
       "      <td>-0.074276</td>\n",
       "      <td>-0.017075</td>\n",
       "      <td>-0.069507</td>\n",
       "      <td>-0.020780</td>\n",
       "      <td>-0.001475</td>\n",
       "      <td>-0.033286</td>\n",
       "      <td>-0.041254</td>\n",
       "      <td>0.015232</td>\n",
       "      <td>0.075466</td>\n",
       "      <td>0.055115</td>\n",
       "      <td>0.017363</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.014622</td>\n",
       "      <td>-0.015154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urinevol2</th>\n",
       "      <td>0.015558</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>0.025737</td>\n",
       "      <td>0.037742</td>\n",
       "      <td>0.020222</td>\n",
       "      <td>0.072229</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>-0.070399</td>\n",
       "      <td>0.025048</td>\n",
       "      <td>0.067772</td>\n",
       "      <td>0.027884</td>\n",
       "      <td>0.048903</td>\n",
       "      <td>0.013975</td>\n",
       "      <td>0.069348</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>0.070360</td>\n",
       "      <td>0.036677</td>\n",
       "      <td>-0.001207</td>\n",
       "      <td>0.144779</td>\n",
       "      <td>0.169825</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877019</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.028836</td>\n",
       "      <td>0.035613</td>\n",
       "      <td>-0.028301</td>\n",
       "      <td>-0.006698</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>-0.044826</td>\n",
       "      <td>-0.008962</td>\n",
       "      <td>-0.048917</td>\n",
       "      <td>0.134741</td>\n",
       "      <td>-0.056226</td>\n",
       "      <td>-0.041638</td>\n",
       "      <td>-0.022060</td>\n",
       "      <td>-0.021329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urineflow2</th>\n",
       "      <td>-0.089875</td>\n",
       "      <td>0.037826</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>0.046641</td>\n",
       "      <td>-0.030979</td>\n",
       "      <td>0.018079</td>\n",
       "      <td>-0.037196</td>\n",
       "      <td>-0.027971</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.035348</td>\n",
       "      <td>0.007310</td>\n",
       "      <td>0.024347</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.041017</td>\n",
       "      <td>-0.011457</td>\n",
       "      <td>0.044166</td>\n",
       "      <td>0.007343</td>\n",
       "      <td>-0.054392</td>\n",
       "      <td>0.134566</td>\n",
       "      <td>0.163706</td>\n",
       "      <td>0.877019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003585</td>\n",
       "      <td>-0.072257</td>\n",
       "      <td>-0.007968</td>\n",
       "      <td>-0.070032</td>\n",
       "      <td>-0.009311</td>\n",
       "      <td>-0.005305</td>\n",
       "      <td>-0.002519</td>\n",
       "      <td>0.036310</td>\n",
       "      <td>-0.065792</td>\n",
       "      <td>0.097990</td>\n",
       "      <td>-0.059972</td>\n",
       "      <td>-0.031535</td>\n",
       "      <td>-0.010723</td>\n",
       "      <td>-0.010191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysmenthlthbad</th>\n",
       "      <td>-0.291180</td>\n",
       "      <td>-0.099976</td>\n",
       "      <td>-0.149445</td>\n",
       "      <td>-0.057821</td>\n",
       "      <td>-0.241684</td>\n",
       "      <td>-0.235660</td>\n",
       "      <td>-0.170142</td>\n",
       "      <td>0.130196</td>\n",
       "      <td>-0.135656</td>\n",
       "      <td>-0.069105</td>\n",
       "      <td>-0.134170</td>\n",
       "      <td>-0.080022</td>\n",
       "      <td>-0.132745</td>\n",
       "      <td>-0.071693</td>\n",
       "      <td>-0.127912</td>\n",
       "      <td>-0.062424</td>\n",
       "      <td>-0.072483</td>\n",
       "      <td>-0.031068</td>\n",
       "      <td>-0.047389</td>\n",
       "      <td>-0.074276</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.003585</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.080094</td>\n",
       "      <td>-0.018282</td>\n",
       "      <td>-0.084800</td>\n",
       "      <td>-0.105961</td>\n",
       "      <td>-0.010269</td>\n",
       "      <td>0.138028</td>\n",
       "      <td>0.074287</td>\n",
       "      <td>-0.074739</td>\n",
       "      <td>-0.103518</td>\n",
       "      <td>0.052169</td>\n",
       "      <td>-0.009252</td>\n",
       "      <td>0.113634</td>\n",
       "      <td>0.113934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npregnancies</th>\n",
       "      <td>0.413983</td>\n",
       "      <td>-0.042527</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>0.025860</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>-0.171957</td>\n",
       "      <td>0.231025</td>\n",
       "      <td>-0.019199</td>\n",
       "      <td>0.112632</td>\n",
       "      <td>0.058770</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.078282</td>\n",
       "      <td>0.108997</td>\n",
       "      <td>0.054395</td>\n",
       "      <td>0.102536</td>\n",
       "      <td>0.052355</td>\n",
       "      <td>0.242473</td>\n",
       "      <td>0.193315</td>\n",
       "      <td>-0.104399</td>\n",
       "      <td>-0.017075</td>\n",
       "      <td>-0.028836</td>\n",
       "      <td>-0.072257</td>\n",
       "      <td>-0.080094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772647</td>\n",
       "      <td>0.853896</td>\n",
       "      <td>-0.009237</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>-0.261658</td>\n",
       "      <td>-0.237206</td>\n",
       "      <td>0.131127</td>\n",
       "      <td>0.029738</td>\n",
       "      <td>-0.122439</td>\n",
       "      <td>-0.051866</td>\n",
       "      <td>0.059746</td>\n",
       "      <td>0.060762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nbabies</th>\n",
       "      <td>0.301199</td>\n",
       "      <td>-0.211471</td>\n",
       "      <td>-0.210000</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>0.025257</td>\n",
       "      <td>-0.143220</td>\n",
       "      <td>0.072117</td>\n",
       "      <td>-0.087352</td>\n",
       "      <td>0.191169</td>\n",
       "      <td>-0.025979</td>\n",
       "      <td>0.203071</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.191697</td>\n",
       "      <td>-0.024730</td>\n",
       "      <td>0.178447</td>\n",
       "      <td>-0.019899</td>\n",
       "      <td>-0.062079</td>\n",
       "      <td>0.063069</td>\n",
       "      <td>-0.078111</td>\n",
       "      <td>-0.069507</td>\n",
       "      <td>0.035613</td>\n",
       "      <td>-0.007968</td>\n",
       "      <td>-0.018282</td>\n",
       "      <td>0.772647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.307515</td>\n",
       "      <td>-0.031591</td>\n",
       "      <td>0.027435</td>\n",
       "      <td>0.021652</td>\n",
       "      <td>-0.053839</td>\n",
       "      <td>-0.024443</td>\n",
       "      <td>-0.094583</td>\n",
       "      <td>-0.144069</td>\n",
       "      <td>-0.079955</td>\n",
       "      <td>0.059427</td>\n",
       "      <td>0.058893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age1stbaby</th>\n",
       "      <td>0.383298</td>\n",
       "      <td>-0.026522</td>\n",
       "      <td>0.026015</td>\n",
       "      <td>0.065734</td>\n",
       "      <td>0.093522</td>\n",
       "      <td>-0.149076</td>\n",
       "      <td>0.191039</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.126684</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>0.131523</td>\n",
       "      <td>0.081537</td>\n",
       "      <td>0.122912</td>\n",
       "      <td>0.055383</td>\n",
       "      <td>0.116311</td>\n",
       "      <td>0.055465</td>\n",
       "      <td>0.201397</td>\n",
       "      <td>0.169215</td>\n",
       "      <td>-0.097208</td>\n",
       "      <td>-0.020780</td>\n",
       "      <td>-0.028301</td>\n",
       "      <td>-0.070032</td>\n",
       "      <td>-0.084800</td>\n",
       "      <td>0.853896</td>\n",
       "      <td>0.307515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011817</td>\n",
       "      <td>-0.002008</td>\n",
       "      <td>-0.237239</td>\n",
       "      <td>-0.198910</td>\n",
       "      <td>0.086549</td>\n",
       "      <td>0.059321</td>\n",
       "      <td>-0.164595</td>\n",
       "      <td>-0.062959</td>\n",
       "      <td>0.058299</td>\n",
       "      <td>0.059426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleephrsnight</th>\n",
       "      <td>-0.014003</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.040207</td>\n",
       "      <td>-0.049063</td>\n",
       "      <td>-0.030406</td>\n",
       "      <td>-0.042921</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>-0.023148</td>\n",
       "      <td>-0.064822</td>\n",
       "      <td>-0.027819</td>\n",
       "      <td>-0.066430</td>\n",
       "      <td>-0.024846</td>\n",
       "      <td>-0.068693</td>\n",
       "      <td>-0.015968</td>\n",
       "      <td>-0.058951</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>-0.010038</td>\n",
       "      <td>-0.036950</td>\n",
       "      <td>-0.001475</td>\n",
       "      <td>-0.006698</td>\n",
       "      <td>-0.009311</td>\n",
       "      <td>-0.105961</td>\n",
       "      <td>-0.009237</td>\n",
       "      <td>-0.031591</td>\n",
       "      <td>-0.011817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002251</td>\n",
       "      <td>-0.042578</td>\n",
       "      <td>0.037084</td>\n",
       "      <td>-0.077798</td>\n",
       "      <td>0.083071</td>\n",
       "      <td>-0.097844</td>\n",
       "      <td>-0.004857</td>\n",
       "      <td>-0.063056</td>\n",
       "      <td>-0.063680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physactivedays</th>\n",
       "      <td>-0.010234</td>\n",
       "      <td>-0.021698</td>\n",
       "      <td>-0.000803</td>\n",
       "      <td>-0.005430</td>\n",
       "      <td>-0.055953</td>\n",
       "      <td>-0.047966</td>\n",
       "      <td>-0.048707</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>-0.017517</td>\n",
       "      <td>-0.034609</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>-0.034676</td>\n",
       "      <td>-0.017795</td>\n",
       "      <td>-0.034842</td>\n",
       "      <td>-0.012373</td>\n",
       "      <td>-0.032469</td>\n",
       "      <td>0.017775</td>\n",
       "      <td>-0.014338</td>\n",
       "      <td>-0.028334</td>\n",
       "      <td>-0.033286</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>-0.005305</td>\n",
       "      <td>-0.010269</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.027435</td>\n",
       "      <td>-0.002008</td>\n",
       "      <td>-0.002251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012356</td>\n",
       "      <td>-0.003423</td>\n",
       "      <td>-0.028371</td>\n",
       "      <td>0.036264</td>\n",
       "      <td>-0.015824</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>-0.004181</td>\n",
       "      <td>-0.003521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcoholday</th>\n",
       "      <td>-0.318159</td>\n",
       "      <td>-0.152383</td>\n",
       "      <td>-0.234888</td>\n",
       "      <td>-0.102930</td>\n",
       "      <td>-0.093658</td>\n",
       "      <td>-0.052635</td>\n",
       "      <td>-0.087767</td>\n",
       "      <td>0.103364</td>\n",
       "      <td>-0.058497</td>\n",
       "      <td>-0.081156</td>\n",
       "      <td>-0.069076</td>\n",
       "      <td>-0.085774</td>\n",
       "      <td>-0.058149</td>\n",
       "      <td>-0.068278</td>\n",
       "      <td>-0.056619</td>\n",
       "      <td>-0.089571</td>\n",
       "      <td>-0.138061</td>\n",
       "      <td>-0.089703</td>\n",
       "      <td>0.028795</td>\n",
       "      <td>-0.041254</td>\n",
       "      <td>-0.044826</td>\n",
       "      <td>-0.002519</td>\n",
       "      <td>0.138028</td>\n",
       "      <td>-0.261658</td>\n",
       "      <td>0.021652</td>\n",
       "      <td>-0.237239</td>\n",
       "      <td>-0.042578</td>\n",
       "      <td>-0.012356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116889</td>\n",
       "      <td>-0.033142</td>\n",
       "      <td>-0.223272</td>\n",
       "      <td>0.137117</td>\n",
       "      <td>0.110688</td>\n",
       "      <td>-0.005677</td>\n",
       "      <td>-0.006615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcoholyear</th>\n",
       "      <td>-0.219312</td>\n",
       "      <td>0.090485</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>0.068513</td>\n",
       "      <td>-0.220016</td>\n",
       "      <td>-0.044471</td>\n",
       "      <td>-0.247641</td>\n",
       "      <td>0.019636</td>\n",
       "      <td>-0.053425</td>\n",
       "      <td>-0.035280</td>\n",
       "      <td>-0.054089</td>\n",
       "      <td>-0.038428</td>\n",
       "      <td>-0.049197</td>\n",
       "      <td>-0.030786</td>\n",
       "      <td>-0.052894</td>\n",
       "      <td>-0.031876</td>\n",
       "      <td>0.083916</td>\n",
       "      <td>-0.029324</td>\n",
       "      <td>0.007543</td>\n",
       "      <td>0.015232</td>\n",
       "      <td>-0.008962</td>\n",
       "      <td>0.036310</td>\n",
       "      <td>0.074287</td>\n",
       "      <td>-0.237206</td>\n",
       "      <td>-0.053839</td>\n",
       "      <td>-0.198910</td>\n",
       "      <td>0.037084</td>\n",
       "      <td>-0.003423</td>\n",
       "      <td>0.116889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.063322</td>\n",
       "      <td>0.009190</td>\n",
       "      <td>0.084297</td>\n",
       "      <td>0.092723</td>\n",
       "      <td>-0.052629</td>\n",
       "      <td>-0.052807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smokeage</th>\n",
       "      <td>0.399126</td>\n",
       "      <td>-0.056427</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>-0.047281</td>\n",
       "      <td>0.286683</td>\n",
       "      <td>0.273081</td>\n",
       "      <td>0.205083</td>\n",
       "      <td>-0.060835</td>\n",
       "      <td>0.208547</td>\n",
       "      <td>0.136479</td>\n",
       "      <td>0.212153</td>\n",
       "      <td>0.144642</td>\n",
       "      <td>0.203273</td>\n",
       "      <td>0.134411</td>\n",
       "      <td>0.205014</td>\n",
       "      <td>0.132276</td>\n",
       "      <td>0.044436</td>\n",
       "      <td>0.101754</td>\n",
       "      <td>0.055175</td>\n",
       "      <td>0.075466</td>\n",
       "      <td>-0.048917</td>\n",
       "      <td>-0.065792</td>\n",
       "      <td>-0.074739</td>\n",
       "      <td>0.131127</td>\n",
       "      <td>-0.024443</td>\n",
       "      <td>0.086549</td>\n",
       "      <td>-0.077798</td>\n",
       "      <td>-0.028371</td>\n",
       "      <td>-0.033142</td>\n",
       "      <td>-0.063322</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.171661</td>\n",
       "      <td>0.270080</td>\n",
       "      <td>0.053533</td>\n",
       "      <td>0.061337</td>\n",
       "      <td>0.062015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sexage</th>\n",
       "      <td>0.225003</td>\n",
       "      <td>0.177402</td>\n",
       "      <td>0.217386</td>\n",
       "      <td>0.104643</td>\n",
       "      <td>-0.045716</td>\n",
       "      <td>-0.050732</td>\n",
       "      <td>-0.025283</td>\n",
       "      <td>-0.005813</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>0.029631</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.042818</td>\n",
       "      <td>0.013093</td>\n",
       "      <td>0.032556</td>\n",
       "      <td>0.005861</td>\n",
       "      <td>0.026423</td>\n",
       "      <td>0.058096</td>\n",
       "      <td>0.085658</td>\n",
       "      <td>-0.023486</td>\n",
       "      <td>0.055115</td>\n",
       "      <td>0.134741</td>\n",
       "      <td>0.097990</td>\n",
       "      <td>-0.103518</td>\n",
       "      <td>0.029738</td>\n",
       "      <td>-0.094583</td>\n",
       "      <td>0.059321</td>\n",
       "      <td>0.083071</td>\n",
       "      <td>0.036264</td>\n",
       "      <td>-0.223272</td>\n",
       "      <td>0.009190</td>\n",
       "      <td>-0.171661</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.449353</td>\n",
       "      <td>-0.201361</td>\n",
       "      <td>-0.046610</td>\n",
       "      <td>-0.045794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sexnumpartnlife</th>\n",
       "      <td>0.074638</td>\n",
       "      <td>-0.054467</td>\n",
       "      <td>-0.025297</td>\n",
       "      <td>-0.047289</td>\n",
       "      <td>0.101659</td>\n",
       "      <td>0.212346</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>-0.054960</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.098724</td>\n",
       "      <td>0.066088</td>\n",
       "      <td>0.114244</td>\n",
       "      <td>0.059108</td>\n",
       "      <td>0.100783</td>\n",
       "      <td>0.059407</td>\n",
       "      <td>0.100719</td>\n",
       "      <td>-0.049657</td>\n",
       "      <td>0.026115</td>\n",
       "      <td>0.053858</td>\n",
       "      <td>0.017363</td>\n",
       "      <td>-0.056226</td>\n",
       "      <td>-0.059972</td>\n",
       "      <td>0.052169</td>\n",
       "      <td>-0.122439</td>\n",
       "      <td>-0.144069</td>\n",
       "      <td>-0.164595</td>\n",
       "      <td>-0.097844</td>\n",
       "      <td>-0.015824</td>\n",
       "      <td>0.137117</td>\n",
       "      <td>0.084297</td>\n",
       "      <td>0.270080</td>\n",
       "      <td>-0.449353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.402384</td>\n",
       "      <td>0.032024</td>\n",
       "      <td>0.032185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sexnumpartyear</th>\n",
       "      <td>-0.199486</td>\n",
       "      <td>-0.036310</td>\n",
       "      <td>-0.051002</td>\n",
       "      <td>-0.015147</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>0.059533</td>\n",
       "      <td>-0.080398</td>\n",
       "      <td>-0.046973</td>\n",
       "      <td>-0.078827</td>\n",
       "      <td>-0.049160</td>\n",
       "      <td>-0.083511</td>\n",
       "      <td>-0.059955</td>\n",
       "      <td>-0.079389</td>\n",
       "      <td>-0.054891</td>\n",
       "      <td>-0.073197</td>\n",
       "      <td>-0.041058</td>\n",
       "      <td>-0.009608</td>\n",
       "      <td>-0.124147</td>\n",
       "      <td>0.033827</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.041638</td>\n",
       "      <td>-0.031535</td>\n",
       "      <td>-0.009252</td>\n",
       "      <td>-0.051866</td>\n",
       "      <td>-0.079955</td>\n",
       "      <td>-0.062959</td>\n",
       "      <td>-0.004857</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.110688</td>\n",
       "      <td>0.092723</td>\n",
       "      <td>0.053533</td>\n",
       "      <td>-0.201361</td>\n",
       "      <td>0.402384</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.067497</td>\n",
       "      <td>-0.069028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flu</th>\n",
       "      <td>0.086335</td>\n",
       "      <td>-0.078384</td>\n",
       "      <td>-0.055360</td>\n",
       "      <td>-0.056579</td>\n",
       "      <td>0.035597</td>\n",
       "      <td>-0.016885</td>\n",
       "      <td>0.061170</td>\n",
       "      <td>0.019463</td>\n",
       "      <td>0.079941</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.079960</td>\n",
       "      <td>0.010591</td>\n",
       "      <td>0.079895</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>0.075550</td>\n",
       "      <td>-0.005755</td>\n",
       "      <td>-0.045022</td>\n",
       "      <td>0.028720</td>\n",
       "      <td>-0.012236</td>\n",
       "      <td>-0.014622</td>\n",
       "      <td>-0.022060</td>\n",
       "      <td>-0.010723</td>\n",
       "      <td>0.113634</td>\n",
       "      <td>0.059746</td>\n",
       "      <td>0.059427</td>\n",
       "      <td>0.058299</td>\n",
       "      <td>-0.063056</td>\n",
       "      <td>-0.004181</td>\n",
       "      <td>-0.005677</td>\n",
       "      <td>-0.052629</td>\n",
       "      <td>0.061337</td>\n",
       "      <td>-0.046610</td>\n",
       "      <td>0.032024</td>\n",
       "      <td>-0.067497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flutype</th>\n",
       "      <td>0.087716</td>\n",
       "      <td>-0.078572</td>\n",
       "      <td>-0.055144</td>\n",
       "      <td>-0.056848</td>\n",
       "      <td>0.036042</td>\n",
       "      <td>-0.016921</td>\n",
       "      <td>0.061793</td>\n",
       "      <td>0.019251</td>\n",
       "      <td>0.081032</td>\n",
       "      <td>0.003202</td>\n",
       "      <td>0.081090</td>\n",
       "      <td>0.010489</td>\n",
       "      <td>0.080962</td>\n",
       "      <td>0.008319</td>\n",
       "      <td>0.076786</td>\n",
       "      <td>-0.005528</td>\n",
       "      <td>-0.044682</td>\n",
       "      <td>0.029138</td>\n",
       "      <td>-0.013221</td>\n",
       "      <td>-0.015154</td>\n",
       "      <td>-0.021329</td>\n",
       "      <td>-0.010191</td>\n",
       "      <td>0.113934</td>\n",
       "      <td>0.060762</td>\n",
       "      <td>0.058893</td>\n",
       "      <td>0.059426</td>\n",
       "      <td>-0.063680</td>\n",
       "      <td>-0.003521</td>\n",
       "      <td>-0.006615</td>\n",
       "      <td>-0.052807</td>\n",
       "      <td>0.062015</td>\n",
       "      <td>-0.045794</td>\n",
       "      <td>0.032185</td>\n",
       "      <td>-0.069028</td>\n",
       "      <td>0.999636</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      age  hhincomemid   poverty  homerooms    weight  \\\n",
       "age              1.000000     0.028933  0.193999   0.034219  0.557354   \n",
       "hhincomemid      0.028933     1.000000  0.894293   0.452291  0.026768   \n",
       "poverty          0.193999     0.894293  1.000000   0.408328  0.104983   \n",
       "homerooms        0.034219     0.452291  0.408328   1.000000 -0.013075   \n",
       "weight           0.557354     0.026768  0.104983  -0.013075  1.000000   \n",
       "height           0.382616     0.127238  0.182484   0.028887  0.687848   \n",
       "bmi              0.472411    -0.048542  0.022712  -0.062440  0.856757   \n",
       "pulse           -0.223376    -0.068457 -0.106283  -0.020503 -0.107407   \n",
       "bpsysave         0.546726    -0.066320  0.022357  -0.026229  0.355787   \n",
       "bpdiaave         0.292232     0.044845  0.082640  -0.037116  0.335541   \n",
       "bpsys1           0.561878    -0.069352  0.023606  -0.024313  0.355865   \n",
       "bpdia1           0.325356     0.041477  0.081067  -0.030235  0.342757   \n",
       "bpsys2           0.540107    -0.064774  0.024528  -0.026956  0.348634   \n",
       "bpdia2           0.295472     0.043117  0.079009  -0.037979  0.336996   \n",
       "bpsys3           0.533722    -0.063799  0.020303  -0.019102  0.348913   \n",
       "bpdia3           0.276902     0.054107  0.088571  -0.027618  0.312529   \n",
       "directchol       0.229572     0.095501  0.123877   0.076992 -0.062876   \n",
       "totchol          0.252555     0.025612  0.064914   0.003695  0.102681   \n",
       "urinevol1       -0.025893     0.049177  0.029671  -0.021917  0.115205   \n",
       "urineflow1       0.097017     0.102159  0.112139   0.016319  0.136562   \n",
       "urinevol2        0.015558     0.038860  0.025737   0.037742  0.020222   \n",
       "urineflow2      -0.089875     0.037826  0.007266   0.046641 -0.030979   \n",
       "daysmenthlthbad -0.291180    -0.099976 -0.149445  -0.057821 -0.241684   \n",
       "npregnancies     0.413983    -0.042527  0.019474   0.025860  0.115900   \n",
       "nbabies          0.301199    -0.211471 -0.210000   0.019830  0.025257   \n",
       "age1stbaby       0.383298    -0.026522  0.026015   0.065734  0.093522   \n",
       "sleephrsnight   -0.014003     0.012346  0.020508   0.040207 -0.049063   \n",
       "physactivedays  -0.010234    -0.021698 -0.000803  -0.005430 -0.055953   \n",
       "alcoholday      -0.318159    -0.152383 -0.234888  -0.102930 -0.093658   \n",
       "alcoholyear     -0.219312     0.090485  0.056185   0.068513 -0.220016   \n",
       "smokeage         0.399126    -0.056427  0.002829  -0.047281  0.286683   \n",
       "sexage           0.225003     0.177402  0.217386   0.104643 -0.045716   \n",
       "sexnumpartnlife  0.074638    -0.054467 -0.025297  -0.047289  0.101659   \n",
       "sexnumpartyear  -0.199486    -0.036310 -0.051002  -0.015147 -0.039159   \n",
       "flu              0.086335    -0.078384 -0.055360  -0.056579  0.035597   \n",
       "flutype          0.087716    -0.078572 -0.055144  -0.056848  0.036042   \n",
       "\n",
       "                   height       bmi     pulse  bpsysave  bpdiaave    bpsys1  \\\n",
       "age              0.382616  0.472411 -0.223376  0.546726  0.292232  0.561878   \n",
       "hhincomemid      0.127238 -0.048542 -0.068457 -0.066320  0.044845 -0.069352   \n",
       "poverty          0.182484  0.022712 -0.106283  0.022357  0.082640  0.023606   \n",
       "homerooms        0.028887 -0.062440 -0.020503 -0.026229 -0.037116 -0.024313   \n",
       "weight           0.687848  0.856757 -0.107407  0.355787  0.335541  0.355865   \n",
       "height           1.000000  0.384586 -0.180858  0.223047  0.262312  0.227286   \n",
       "bmi              0.384586  1.000000 -0.055185  0.336773  0.280516  0.334985   \n",
       "pulse           -0.180858 -0.055185  1.000000 -0.124058  0.012708 -0.131726   \n",
       "bpsysave         0.223047  0.336773 -0.124058  1.000000  0.460086  0.941664   \n",
       "bpdiaave         0.262312  0.280516  0.012708  0.460086  1.000000  0.444820   \n",
       "bpsys1           0.227286  0.334985 -0.131726  0.941664  0.444820  1.000000   \n",
       "bpdia1           0.262777  0.293164  0.011626  0.486047  0.904659  0.470708   \n",
       "bpsys2           0.213104  0.331414 -0.121916  0.985708  0.451284  0.935021   \n",
       "bpdia2           0.259023  0.282617  0.008657  0.461620  0.975596  0.447532   \n",
       "bpsys3           0.226284  0.327302 -0.116744  0.985168  0.452844  0.921360   \n",
       "bpdia3           0.254311  0.257359  0.013131  0.440902  0.976032  0.423749   \n",
       "directchol      -0.008445 -0.142773 -0.048940 -0.043118 -0.050705 -0.034462   \n",
       "totchol          0.021482  0.165600 -0.017322  0.231903  0.263140  0.229614   \n",
       "urinevol1        0.178908  0.050688 -0.064175  0.025573  0.087436  0.028084   \n",
       "urineflow1       0.185074  0.078677 -0.094796  0.099369  0.130654  0.099326   \n",
       "urinevol2        0.072229  0.001737 -0.070399  0.025048  0.067772  0.027884   \n",
       "urineflow2       0.018079 -0.037196 -0.027971  0.000329  0.035348  0.007310   \n",
       "daysmenthlthbad -0.235660 -0.170142  0.130196 -0.135656 -0.069105 -0.134170   \n",
       "npregnancies    -0.171957  0.231025 -0.019199  0.112632  0.058770  0.122671   \n",
       "nbabies         -0.143220  0.072117 -0.087352  0.191169 -0.025979  0.203071   \n",
       "age1stbaby      -0.149076  0.191039 -0.037938  0.126684  0.059261  0.131523   \n",
       "sleephrsnight   -0.030406 -0.042921  0.006600 -0.023148 -0.064822 -0.027819   \n",
       "physactivedays  -0.047966 -0.048707  0.006073 -0.017517 -0.034609 -0.007902   \n",
       "alcoholday      -0.052635 -0.087767  0.103364 -0.058497 -0.081156 -0.069076   \n",
       "alcoholyear     -0.044471 -0.247641  0.019636 -0.053425 -0.035280 -0.054089   \n",
       "smokeage         0.273081  0.205083 -0.060835  0.208547  0.136479  0.212153   \n",
       "sexage          -0.050732 -0.025283 -0.005813  0.007250  0.029631  0.000738   \n",
       "sexnumpartnlife  0.212346  0.002204 -0.054960  0.059105  0.098724  0.066088   \n",
       "sexnumpartyear   0.059533 -0.080398 -0.046973 -0.078827 -0.049160 -0.083511   \n",
       "flu             -0.016885  0.061170  0.019463  0.079941  0.003045  0.079960   \n",
       "flutype         -0.016921  0.061793  0.019251  0.081032  0.003202  0.081090   \n",
       "\n",
       "                   bpdia1    bpsys2    bpdia2    bpsys3    bpdia3  directchol  \\\n",
       "age              0.325356  0.540107  0.295472  0.533722  0.276902    0.229572   \n",
       "hhincomemid      0.041477 -0.064774  0.043117 -0.063799  0.054107    0.095501   \n",
       "poverty          0.081067  0.024528  0.079009  0.020303  0.088571    0.123877   \n",
       "homerooms       -0.030235 -0.026956 -0.037979 -0.019102 -0.027618    0.076992   \n",
       "weight           0.342757  0.348634  0.336996  0.348913  0.312529   -0.062876   \n",
       "height           0.262777  0.213104  0.259023  0.226284  0.254311   -0.008445   \n",
       "bmi              0.293164  0.331414  0.282617  0.327302  0.257359   -0.142773   \n",
       "pulse            0.011626 -0.121916  0.008657 -0.116744  0.013131   -0.048940   \n",
       "bpsysave         0.486047  0.985708  0.461620  0.985168  0.440902   -0.043118   \n",
       "bpdiaave         0.904659  0.451284  0.975596  0.452844  0.976032   -0.050705   \n",
       "bpsys1           0.470708  0.935021  0.447532  0.921360  0.423749   -0.034462   \n",
       "bpdia1           1.000000  0.480138  0.890017  0.472029  0.877586   -0.043100   \n",
       "bpsys2           0.480138  1.000000  0.455221  0.941570  0.432317   -0.039475   \n",
       "bpdia2           0.890017  0.455221  1.000000  0.454356  0.905536   -0.048538   \n",
       "bpsys3           0.472029  0.941570  0.454356  1.000000  0.436190   -0.043418   \n",
       "bpdia3           0.877586  0.432317  0.905536  0.436190  1.000000   -0.043060   \n",
       "directchol      -0.043100 -0.039475 -0.048538 -0.043418 -0.043060    1.000000   \n",
       "totchol          0.272627  0.229129  0.264022  0.228684  0.252889    0.079563   \n",
       "urinevol1        0.070533  0.029282  0.086022  0.022178  0.088967   -0.041384   \n",
       "urineflow1       0.125160  0.105439  0.119889  0.092663  0.133571    0.005343   \n",
       "urinevol2        0.048903  0.013975  0.069348  0.012137  0.070360    0.036677   \n",
       "urineflow2       0.024347  0.000017  0.041017 -0.011457  0.044166    0.007343   \n",
       "daysmenthlthbad -0.080022 -0.132745 -0.071693 -0.127912 -0.062424   -0.072483   \n",
       "npregnancies     0.078282  0.108997  0.054395  0.102536  0.052355    0.242473   \n",
       "nbabies          0.000309  0.191697 -0.024730  0.178447 -0.019899   -0.062079   \n",
       "age1stbaby       0.081537  0.122912  0.055383  0.116311  0.055465    0.201397   \n",
       "sleephrsnight   -0.066430 -0.024846 -0.068693 -0.015968 -0.058951    0.001058   \n",
       "physactivedays  -0.034676 -0.017795 -0.034842 -0.012373 -0.032469    0.017775   \n",
       "alcoholday      -0.085774 -0.058149 -0.068278 -0.056619 -0.089571   -0.138061   \n",
       "alcoholyear     -0.038428 -0.049197 -0.030786 -0.052894 -0.031876    0.083916   \n",
       "smokeage         0.144642  0.203273  0.134411  0.205014  0.132276    0.044436   \n",
       "sexage           0.042818  0.013093  0.032556  0.005861  0.026423    0.058096   \n",
       "sexnumpartnlife  0.114244  0.059108  0.100783  0.059407  0.100719   -0.049657   \n",
       "sexnumpartyear  -0.059955 -0.079389 -0.054891 -0.073197 -0.041058   -0.009608   \n",
       "flu              0.010591  0.079895  0.008206  0.075550 -0.005755   -0.045022   \n",
       "flutype          0.010489  0.080962  0.008319  0.076786 -0.005528   -0.044682   \n",
       "\n",
       "                  totchol  urinevol1  urineflow1  urinevol2  urineflow2  \\\n",
       "age              0.252555  -0.025893    0.097017   0.015558   -0.089875   \n",
       "hhincomemid      0.025612   0.049177    0.102159   0.038860    0.037826   \n",
       "poverty          0.064914   0.029671    0.112139   0.025737    0.007266   \n",
       "homerooms        0.003695  -0.021917    0.016319   0.037742    0.046641   \n",
       "weight           0.102681   0.115205    0.136562   0.020222   -0.030979   \n",
       "height           0.021482   0.178908    0.185074   0.072229    0.018079   \n",
       "bmi              0.165600   0.050688    0.078677   0.001737   -0.037196   \n",
       "pulse           -0.017322  -0.064175   -0.094796  -0.070399   -0.027971   \n",
       "bpsysave         0.231903   0.025573    0.099369   0.025048    0.000329   \n",
       "bpdiaave         0.263140   0.087436    0.130654   0.067772    0.035348   \n",
       "bpsys1           0.229614   0.028084    0.099326   0.027884    0.007310   \n",
       "bpdia1           0.272627   0.070533    0.125160   0.048903    0.024347   \n",
       "bpsys2           0.229129   0.029282    0.105439   0.013975    0.000017   \n",
       "bpdia2           0.264022   0.086022    0.119889   0.069348    0.041017   \n",
       "bpsys3           0.228684   0.022178    0.092663   0.012137   -0.011457   \n",
       "bpdia3           0.252889   0.088967    0.133571   0.070360    0.044166   \n",
       "directchol       0.079563  -0.041384    0.005343   0.036677    0.007343   \n",
       "totchol          1.000000   0.007657    0.070741  -0.001207   -0.054392   \n",
       "urinevol1        0.007657   1.000000    0.713369   0.144779    0.134566   \n",
       "urineflow1       0.070741   0.713369    1.000000   0.169825    0.163706   \n",
       "urinevol2       -0.001207   0.144779    0.169825   1.000000    0.877019   \n",
       "urineflow2      -0.054392   0.134566    0.163706   0.877019    1.000000   \n",
       "daysmenthlthbad -0.031068  -0.047389   -0.074276  -0.048209   -0.003585   \n",
       "npregnancies     0.193315  -0.104399   -0.017075  -0.028836   -0.072257   \n",
       "nbabies          0.063069  -0.078111   -0.069507   0.035613   -0.007968   \n",
       "age1stbaby       0.169215  -0.097208   -0.020780  -0.028301   -0.070032   \n",
       "sleephrsnight   -0.010038  -0.036950   -0.001475  -0.006698   -0.009311   \n",
       "physactivedays  -0.014338  -0.028334   -0.033286   0.014290   -0.005305   \n",
       "alcoholday      -0.089703   0.028795   -0.041254  -0.044826   -0.002519   \n",
       "alcoholyear     -0.029324   0.007543    0.015232  -0.008962    0.036310   \n",
       "smokeage         0.101754   0.055175    0.075466  -0.048917   -0.065792   \n",
       "sexage           0.085658  -0.023486    0.055115   0.134741    0.097990   \n",
       "sexnumpartnlife  0.026115   0.053858    0.017363  -0.056226   -0.059972   \n",
       "sexnumpartyear  -0.124147   0.033827   -0.001457  -0.041638   -0.031535   \n",
       "flu              0.028720  -0.012236   -0.014622  -0.022060   -0.010723   \n",
       "flutype          0.029138  -0.013221   -0.015154  -0.021329   -0.010191   \n",
       "\n",
       "                 daysmenthlthbad  npregnancies   nbabies  age1stbaby  \\\n",
       "age                    -0.291180      0.413983  0.301199    0.383298   \n",
       "hhincomemid            -0.099976     -0.042527 -0.211471   -0.026522   \n",
       "poverty                -0.149445      0.019474 -0.210000    0.026015   \n",
       "homerooms              -0.057821      0.025860  0.019830    0.065734   \n",
       "weight                 -0.241684      0.115900  0.025257    0.093522   \n",
       "height                 -0.235660     -0.171957 -0.143220   -0.149076   \n",
       "bmi                    -0.170142      0.231025  0.072117    0.191039   \n",
       "pulse                   0.130196     -0.019199 -0.087352   -0.037938   \n",
       "bpsysave               -0.135656      0.112632  0.191169    0.126684   \n",
       "bpdiaave               -0.069105      0.058770 -0.025979    0.059261   \n",
       "bpsys1                 -0.134170      0.122671  0.203071    0.131523   \n",
       "bpdia1                 -0.080022      0.078282  0.000309    0.081537   \n",
       "bpsys2                 -0.132745      0.108997  0.191697    0.122912   \n",
       "bpdia2                 -0.071693      0.054395 -0.024730    0.055383   \n",
       "bpsys3                 -0.127912      0.102536  0.178447    0.116311   \n",
       "bpdia3                 -0.062424      0.052355 -0.019899    0.055465   \n",
       "directchol             -0.072483      0.242473 -0.062079    0.201397   \n",
       "totchol                -0.031068      0.193315  0.063069    0.169215   \n",
       "urinevol1              -0.047389     -0.104399 -0.078111   -0.097208   \n",
       "urineflow1             -0.074276     -0.017075 -0.069507   -0.020780   \n",
       "urinevol2              -0.048209     -0.028836  0.035613   -0.028301   \n",
       "urineflow2             -0.003585     -0.072257 -0.007968   -0.070032   \n",
       "daysmenthlthbad         1.000000     -0.080094 -0.018282   -0.084800   \n",
       "npregnancies           -0.080094      1.000000  0.772647    0.853896   \n",
       "nbabies                -0.018282      0.772647  1.000000    0.307515   \n",
       "age1stbaby             -0.084800      0.853896  0.307515    1.000000   \n",
       "sleephrsnight          -0.105961     -0.009237 -0.031591   -0.011817   \n",
       "physactivedays         -0.010269      0.004921  0.027435   -0.002008   \n",
       "alcoholday              0.138028     -0.261658  0.021652   -0.237239   \n",
       "alcoholyear             0.074287     -0.237206 -0.053839   -0.198910   \n",
       "smokeage               -0.074739      0.131127 -0.024443    0.086549   \n",
       "sexage                 -0.103518      0.029738 -0.094583    0.059321   \n",
       "sexnumpartnlife         0.052169     -0.122439 -0.144069   -0.164595   \n",
       "sexnumpartyear         -0.009252     -0.051866 -0.079955   -0.062959   \n",
       "flu                     0.113634      0.059746  0.059427    0.058299   \n",
       "flutype                 0.113934      0.060762  0.058893    0.059426   \n",
       "\n",
       "                 sleephrsnight  physactivedays  alcoholday  alcoholyear  \\\n",
       "age                  -0.014003       -0.010234   -0.318159    -0.219312   \n",
       "hhincomemid           0.012346       -0.021698   -0.152383     0.090485   \n",
       "poverty               0.020508       -0.000803   -0.234888     0.056185   \n",
       "homerooms             0.040207       -0.005430   -0.102930     0.068513   \n",
       "weight               -0.049063       -0.055953   -0.093658    -0.220016   \n",
       "height               -0.030406       -0.047966   -0.052635    -0.044471   \n",
       "bmi                  -0.042921       -0.048707   -0.087767    -0.247641   \n",
       "pulse                 0.006600        0.006073    0.103364     0.019636   \n",
       "bpsysave             -0.023148       -0.017517   -0.058497    -0.053425   \n",
       "bpdiaave             -0.064822       -0.034609   -0.081156    -0.035280   \n",
       "bpsys1               -0.027819       -0.007902   -0.069076    -0.054089   \n",
       "bpdia1               -0.066430       -0.034676   -0.085774    -0.038428   \n",
       "bpsys2               -0.024846       -0.017795   -0.058149    -0.049197   \n",
       "bpdia2               -0.068693       -0.034842   -0.068278    -0.030786   \n",
       "bpsys3               -0.015968       -0.012373   -0.056619    -0.052894   \n",
       "bpdia3               -0.058951       -0.032469   -0.089571    -0.031876   \n",
       "directchol            0.001058        0.017775   -0.138061     0.083916   \n",
       "totchol              -0.010038       -0.014338   -0.089703    -0.029324   \n",
       "urinevol1            -0.036950       -0.028334    0.028795     0.007543   \n",
       "urineflow1           -0.001475       -0.033286   -0.041254     0.015232   \n",
       "urinevol2            -0.006698        0.014290   -0.044826    -0.008962   \n",
       "urineflow2           -0.009311       -0.005305   -0.002519     0.036310   \n",
       "daysmenthlthbad      -0.105961       -0.010269    0.138028     0.074287   \n",
       "npregnancies         -0.009237        0.004921   -0.261658    -0.237206   \n",
       "nbabies              -0.031591        0.027435    0.021652    -0.053839   \n",
       "age1stbaby           -0.011817       -0.002008   -0.237239    -0.198910   \n",
       "sleephrsnight         1.000000       -0.002251   -0.042578     0.037084   \n",
       "physactivedays       -0.002251        1.000000   -0.012356    -0.003423   \n",
       "alcoholday           -0.042578       -0.012356    1.000000     0.116889   \n",
       "alcoholyear           0.037084       -0.003423    0.116889     1.000000   \n",
       "smokeage             -0.077798       -0.028371   -0.033142    -0.063322   \n",
       "sexage                0.083071        0.036264   -0.223272     0.009190   \n",
       "sexnumpartnlife      -0.097844       -0.015824    0.137117     0.084297   \n",
       "sexnumpartyear       -0.004857        0.002651    0.110688     0.092723   \n",
       "flu                  -0.063056       -0.004181   -0.005677    -0.052629   \n",
       "flutype              -0.063680       -0.003521   -0.006615    -0.052807   \n",
       "\n",
       "                 smokeage    sexage  sexnumpartnlife  sexnumpartyear  \\\n",
       "age              0.399126  0.225003         0.074638       -0.199486   \n",
       "hhincomemid     -0.056427  0.177402        -0.054467       -0.036310   \n",
       "poverty          0.002829  0.217386        -0.025297       -0.051002   \n",
       "homerooms       -0.047281  0.104643        -0.047289       -0.015147   \n",
       "weight           0.286683 -0.045716         0.101659       -0.039159   \n",
       "height           0.273081 -0.050732         0.212346        0.059533   \n",
       "bmi              0.205083 -0.025283         0.002204       -0.080398   \n",
       "pulse           -0.060835 -0.005813        -0.054960       -0.046973   \n",
       "bpsysave         0.208547  0.007250         0.059105       -0.078827   \n",
       "bpdiaave         0.136479  0.029631         0.098724       -0.049160   \n",
       "bpsys1           0.212153  0.000738         0.066088       -0.083511   \n",
       "bpdia1           0.144642  0.042818         0.114244       -0.059955   \n",
       "bpsys2           0.203273  0.013093         0.059108       -0.079389   \n",
       "bpdia2           0.134411  0.032556         0.100783       -0.054891   \n",
       "bpsys3           0.205014  0.005861         0.059407       -0.073197   \n",
       "bpdia3           0.132276  0.026423         0.100719       -0.041058   \n",
       "directchol       0.044436  0.058096        -0.049657       -0.009608   \n",
       "totchol          0.101754  0.085658         0.026115       -0.124147   \n",
       "urinevol1        0.055175 -0.023486         0.053858        0.033827   \n",
       "urineflow1       0.075466  0.055115         0.017363       -0.001457   \n",
       "urinevol2       -0.048917  0.134741        -0.056226       -0.041638   \n",
       "urineflow2      -0.065792  0.097990        -0.059972       -0.031535   \n",
       "daysmenthlthbad -0.074739 -0.103518         0.052169       -0.009252   \n",
       "npregnancies     0.131127  0.029738        -0.122439       -0.051866   \n",
       "nbabies         -0.024443 -0.094583        -0.144069       -0.079955   \n",
       "age1stbaby       0.086549  0.059321        -0.164595       -0.062959   \n",
       "sleephrsnight   -0.077798  0.083071        -0.097844       -0.004857   \n",
       "physactivedays  -0.028371  0.036264        -0.015824        0.002651   \n",
       "alcoholday      -0.033142 -0.223272         0.137117        0.110688   \n",
       "alcoholyear     -0.063322  0.009190         0.084297        0.092723   \n",
       "smokeage         1.000000 -0.171661         0.270080        0.053533   \n",
       "sexage          -0.171661  1.000000        -0.449353       -0.201361   \n",
       "sexnumpartnlife  0.270080 -0.449353         1.000000        0.402384   \n",
       "sexnumpartyear   0.053533 -0.201361         0.402384        1.000000   \n",
       "flu              0.061337 -0.046610         0.032024       -0.067497   \n",
       "flutype          0.062015 -0.045794         0.032185       -0.069028   \n",
       "\n",
       "                      flu   flutype  \n",
       "age              0.086335  0.087716  \n",
       "hhincomemid     -0.078384 -0.078572  \n",
       "poverty         -0.055360 -0.055144  \n",
       "homerooms       -0.056579 -0.056848  \n",
       "weight           0.035597  0.036042  \n",
       "height          -0.016885 -0.016921  \n",
       "bmi              0.061170  0.061793  \n",
       "pulse            0.019463  0.019251  \n",
       "bpsysave         0.079941  0.081032  \n",
       "bpdiaave         0.003045  0.003202  \n",
       "bpsys1           0.079960  0.081090  \n",
       "bpdia1           0.010591  0.010489  \n",
       "bpsys2           0.079895  0.080962  \n",
       "bpdia2           0.008206  0.008319  \n",
       "bpsys3           0.075550  0.076786  \n",
       "bpdia3          -0.005755 -0.005528  \n",
       "directchol      -0.045022 -0.044682  \n",
       "totchol          0.028720  0.029138  \n",
       "urinevol1       -0.012236 -0.013221  \n",
       "urineflow1      -0.014622 -0.015154  \n",
       "urinevol2       -0.022060 -0.021329  \n",
       "urineflow2      -0.010723 -0.010191  \n",
       "daysmenthlthbad  0.113634  0.113934  \n",
       "npregnancies     0.059746  0.060762  \n",
       "nbabies          0.059427  0.058893  \n",
       "age1stbaby       0.058299  0.059426  \n",
       "sleephrsnight   -0.063056 -0.063680  \n",
       "physactivedays  -0.004181 -0.003521  \n",
       "alcoholday      -0.005677 -0.006615  \n",
       "alcoholyear     -0.052629 -0.052807  \n",
       "smokeage         0.061337  0.062015  \n",
       "sexage          -0.046610 -0.045794  \n",
       "sexnumpartnlife  0.032024  0.032185  \n",
       "sexnumpartyear  -0.067497 -0.069028  \n",
       "flu              1.000000  0.999636  \n",
       "flutype          0.999636  1.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of heavaly correlated features. We are excluding them, as they could pose a problem for some of the algorithms and we have a lot of variables and not that many observations. With this situation we could be faced with a problem of \"curse of dimensionality\". Features that are heavily correlated, but have more NA's or less explanatory power with the targed variable (linear correlation) are excluded. An alternative would be to use PCA, however, this has a linearity assumtion and it would also make it harder to understand the model. However, knowing the most important factors (variable importance) could be very helpful for the application of the model in practice.\n",
    "\n",
    "#### Heavily correlated\n",
    "\n",
    "** poverty & hhincomeid**\n",
    "Heavily correlated, but hhincomemid has a higher correlation with the target variable.\n",
    "\n",
    "** weight & bmi **\n",
    "They are heavaly correlated, however, bmi is a better variable in explaining the target variable.\n",
    "\n",
    "** nbabies, age1stbaby & npregnancies **\n",
    "Correlated. npregnancies explained more in the targed variable which means that the nbabies and age1stbaby is droped.\n",
    "\n",
    "** height & testosterone **\n",
    "Correlated, however, testosterone explaines more.\n",
    "\n",
    "** hhincome & hhincomemid **\n",
    "hhincom is the categorical derivative from hhincomeid, we are droping hhincome. \n",
    "\n",
    "** bmi & bmi_who **\n",
    "bmi_who is the categorical derivative of bmi, we are droping bmi_who\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df flu\n",
    "#-------\n",
    "# Drop variables \n",
    "drop_var = [\"poverty\", \"weight\", \"nbabies\", \"age1stbaby\", \"height\", \"hhincome\", \"bmi_who\"]\n",
    "keep_var = df_flu.columns[~df_flu.columns.isin(drop_var)]\n",
    "df_flu = df_flu[keep_var]\n",
    "\n",
    "# df flu fin\n",
    "#-----------\n",
    "# Drop variables \n",
    "drop_var = [\"poverty\", \"weight\", \"nbabies\", \"age1stbaby\", \"height\", \"hhincome\", \"bmi_who\"]\n",
    "keep_var = df_flu_fin.columns[~df_flu_fin.columns.isin(drop_var)]\n",
    "df_flu_fin = df_flu_fin[keep_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is further a bp cluster of variables where a new feature could be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance of the bp pca [ 0.66599963  0.26943035  0.02163599  0.01665446  0.01381361]\n",
      "explained variance of the urine pca [ 0.859  0.141  0.   ]\n",
      "explained variance of the sexlife pca [ 0.959  0.04   0.001]\n"
     ]
    }
   ],
   "source": [
    "## Create new variables of the heavily correlated variables \n",
    "\n",
    "# df flu\n",
    "#-------\n",
    "# Correlations\n",
    "bp_corr = [\"bpsysave\", \"bpdiaave\", \"bpsys1\", \"bpdia1\", \"bpsys2\", \"bpdia2\", \"bpsys3\", \"bpdia3\"]\n",
    "urine_corr = [\"urinevol1\", \"urineflow1\", \"urinevol2\", \"urineflow2\"]\n",
    "sexlife_corr = [\"sexage\", \"sexnumpartnlife\", \"sexnumpartyear\"]\n",
    "\n",
    "# Extract df's\n",
    "df_corr_bp = df_flu[bp_corr]\n",
    "df_corr_urine = df_flu[urine_corr]\n",
    "df_corr_sexlife = df_flu[sexlife_corr]\n",
    "\n",
    "# Fill na with either mean or zeros\n",
    "df_corr_bp = df_corr_bp.fillna(df_corr_bp.mean())\n",
    "df_corr_urine = df_corr_urine.fillna(df_corr_urine.mean())\n",
    "df_flu.loc[df_flu.sexage.isnull(), 'sexage'] = 0      # Zero is reasnable here\n",
    "df_corr_sexlife = df_corr_sexlife.fillna(df_corr_sexlife.mean())\n",
    "\n",
    "#Apply PCA to data and get the top 3 axes of maximum variation\n",
    "pca_bp = PCA(n_components=5).fit(df_corr_bp)\n",
    "pca_urine = PCA(n_components=3).fit(df_corr_urine)\n",
    "pca_sexlife = PCA(n_components=3).fit(df_corr_sexlife)\n",
    "\n",
    "# Printing\n",
    "#---------\n",
    "print \"explained variance of the bp pca\", pca_bp.explained_variance_ratio_\n",
    "print \"explained variance of the urine pca\", np.round(pca_urine.explained_variance_ratio_, 3)\n",
    "print \"explained variance of the sexlife pca\", np.round(pca_sexlife.explained_variance_ratio_, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen, that the first principal component explaines in the case of bp almost 88.6 % and in the case of urine 77% and in the case of sexlife 96% of the variablity. We are only using this component for the further modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use 1 principal component\n",
    "pca_bp = PCA(n_components=2).fit(df_corr_bp)\n",
    "pca_urine = PCA(n_components=1).fit(df_corr_urine)\n",
    "pca_sexlife = PCA(n_components=1).fit(df_corr_sexlife)\n",
    "\n",
    "# Transform variables\n",
    "bp_pca_vars = pca_bp.transform(df_corr_bp)\n",
    "urine_pca_vars = pca_urine.transform(df_corr_urine)\n",
    "sexlife_pca_vars = pca_sexlife.transform(df_corr_sexlife)\n",
    "\n",
    "# Remove values and add principal component\n",
    "keep_var = df_flu.columns[~df_flu.columns.isin(bp_corr)]\n",
    "df_flu = df_flu[keep_var]\n",
    "keep_var = df_flu.columns[~df_flu.columns.isin(urine_corr)]\n",
    "df_flu = df_flu[keep_var]\n",
    "keep_var = df_flu.columns[~df_flu.columns.isin(sexlife_corr)]\n",
    "df_flu = df_flu[keep_var]\n",
    "\n",
    "df_flu['bp_pca1_vars'] = pd.DataFrame(bp_pca_vars)[0]\n",
    "df_flu['bp_pca2_vars'] = pd.DataFrame(bp_pca_vars)[1]\n",
    "df_flu['urine_pca_vars'] = urine_pca_vars\n",
    "df_flu['sexlife_pca_vars'] = sexlife_pca_vars\n",
    "\n",
    "# df flu fin\n",
    "#-----------\n",
    "# Extract df's\n",
    "df_corr_bp = df_flu_fin[bp_corr]\n",
    "df_corr_urine = df_flu_fin[urine_corr]\n",
    "df_corr_sexlife = df_flu_fin[sexlife_corr]\n",
    "\n",
    "# Fill na with either mean or zeros\n",
    "df_corr_bp = df_corr_bp.fillna(df_corr_bp.mean())\n",
    "df_corr_urine = df_corr_urine.fillna(df_corr_urine.mean())\n",
    "df_flu_fin.loc[df_flu_fin.sexage.isnull(), 'sexage'] = 0      # Zero is reasnable here\n",
    "df_corr_sexlife = df_corr_sexlife.fillna(df_corr_sexlife.mean())\n",
    "\n",
    "# Use 1 principal component\n",
    "pca_bp = PCA(n_components=2).fit(df_corr_bp)\n",
    "pca_urine = PCA(n_components=1).fit(df_corr_urine)\n",
    "pca_sexlife = PCA(n_components=1).fit(df_corr_sexlife)\n",
    "\n",
    "# Transform variables\n",
    "bp_pca_vars = pca_bp.transform(df_corr_bp)\n",
    "urine_pca_vars = pca_urine.transform(df_corr_urine)\n",
    "sexlife_pca_vars = pca_sexlife.transform(df_corr_sexlife)\n",
    "\n",
    "# Remove values and add principal component\n",
    "keep_var = df_flu_fin.columns[~df_flu_fin.columns.isin(bp_corr)]\n",
    "df_flu_fin = df_flu_fin[keep_var]\n",
    "keep_var = df_flu_fin.columns[~df_flu_fin.columns.isin(urine_corr)]\n",
    "df_flu_fin = df_flu_fin[keep_var]\n",
    "keep_var = df_flu_fin.columns[~df_flu_fin.columns.isin(sexlife_corr)]\n",
    "df_flu_fin = df_flu_fin[keep_var]\n",
    "\n",
    "df_flu_fin['bp_pca1_vars'] = pd.DataFrame(bp_pca_vars)[0]\n",
    "df_flu_fin['bp_pca2_vars'] = pd.DataFrame(bp_pca_vars)[1]\n",
    "df_flu_fin['urine_pca_vars'] = urine_pca_vars\n",
    "df_flu_fin['sexlife_pca_vars'] = sexlife_pca_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df flu\n",
    "#-------\n",
    "# Extract variable names\n",
    "var_names = df_flu.columns.values\n",
    "\n",
    "# List of columns to be converted to floating point\n",
    "cat_var = df_flu.dtypes[df_flu.dtypes == \"object\"].index.values\n",
    "num_var = var_names[~df_flu.columns.isin(cat_var)]\n",
    "\n",
    "# Extract variables from data\n",
    "x = df_flu[var_names[(var_names != 'flu') & (var_names != 'flutype')]]\n",
    "y1 = df_flu.flu.values\n",
    "y2 = df_flu.flutype.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract variable names\n",
    "var_names = x.columns.values\n",
    "\n",
    "# Get num index\n",
    "num_index = np.array(range(0, len(var_names)))[x.columns.isin(num_var)]\n",
    "\n",
    "# List of columns to be converted to floating point\n",
    "cat_var = x.dtypes[x.dtypes == \"object\"].index.values\n",
    "num_var = var_names[~x.columns.isin(cat_var)]\n",
    "\n",
    "# Add quadratic numeric effects\n",
    "x_num = x[num_var].values\n",
    "x_num_quad = preprocessing.PolynomialFeatures(degree = 2).fit_transform(x_num)\n",
    "x_cat = pd.get_dummies(x[cat_var], sparse=True).values\n",
    "\n",
    "#quad_fit = pd.DataFrame(quad_features.fit_transform(x[num_var]))\n",
    "x = np.hstack((x_num, x_cat))\n",
    "x_quad = np.hstack((x_num_quad, x_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df flu fin\n",
    "#-----------\n",
    "# Extract variable names\n",
    "var_names = df_flu_fin.columns.values\n",
    "\n",
    "# List of columns to be converted to floating point\n",
    "cat_var = df_flu_fin.dtypes[df_flu_fin.dtypes == \"object\"].index.values\n",
    "num_var = var_names[~df_flu_fin.columns.isin(cat_var)]\n",
    "\n",
    "# Extract variables from data\n",
    "x_fin = df_flu_fin[var_names[(var_names != 'flu') & (var_names != 'flutype')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract variable names\n",
    "var_names = x_fin.columns.values\n",
    "\n",
    "# Get num index\n",
    "num_index = np.array(range(0, len(var_names)))[x_fin.columns.isin(num_var)]\n",
    "\n",
    "# List of columns to be converted to floating point\n",
    "cat_var = x_fin.dtypes[x_fin.dtypes == \"object\"].index.values\n",
    "num_var = var_names[~x_fin.columns.isin(cat_var)]\n",
    "\n",
    "# Add quadratic numeric effects\n",
    "x_num = x_fin[num_var].values\n",
    "x_cat = pd.get_dummies(x_fin[cat_var], sparse=True).values\n",
    "\n",
    "#quad_fit = pd.DataFrame(quad_features.fit_transform(x[num_var]))\n",
    "x_fin = np.hstack((x_num, x_cat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our dataset into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234) # Set random seed\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y1, test_size=0.25)\n",
    "\n",
    "np.random.seed(1234) # Set random seed\n",
    "x_train_quad, x_test_quad, y_train_quad, y_test_quad = train_test_split(x_quad, y1, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3934L, 113L)\n",
      "(3934L, 285L)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print x_train_quad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Establish the Baseline Models\n",
    "\n",
    "What are the baseline models in this case? We can check off three basic models: \n",
    "\n",
    "1. a model that labels everything 1\n",
    "2. a model that labels everything 0\n",
    "3. a model that randomly guesses a label, 1 or 0\n",
    "\n",
    "Before implementing anything fancy, let's implement these baseline models and see how they do.\n",
    "\n",
    "**Note:** Again, think about accuracy in a **meaningful** way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function for computing the accuracy a given model on the entire test set, the accuracy on class 0 in the test set\n",
    "#and the accuracy on class 1\n",
    "score = lambda model, x_test, y_test: pd.Series([model.score(x_test, y_test), \n",
    "                                                 model.score(x_test[y_test==0], y_test[y_test==0]),\n",
    "                                                 model.score(x_test[y_test==1], y_test[y_test==1])],\n",
    "                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A model that labels everything 1\n",
    "class Pos_model(object):\n",
    "    def predict(self, x):\n",
    "        return np.array([1] * len(x))\n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)\n",
    "    \n",
    "# A model that labels everything 0\n",
    "class Neg_model(object):\n",
    "    def predict(self, x):\n",
    "        return np.array([0] * len(x))\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)\n",
    "\n",
    "\n",
    "# A model that randomly labels things\n",
    "class Random_model(object):\n",
    "    def predict(self, x):\n",
    "        return np.random.randint(0, 2, len(x))\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_model = Pos_model()\n",
    "pos_model_scores = score(pos_model, x_test, y_test)\n",
    "\n",
    "neg_model = Neg_model()\n",
    "neg_model_scores = score(neg_model, x_test, y_test)\n",
    "\n",
    "random_model = Random_model()\n",
    "random_model_scores = score(random_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg model</th>\n",
       "      <th>pos model</th>\n",
       "      <th>random model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall accuracy</th>\n",
       "      <td>0.945122</td>\n",
       "      <td>0.054878</td>\n",
       "      <td>0.507622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.486290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.430556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     neg model  pos model  random model\n",
       "overall accuracy      0.945122   0.054878      0.507622\n",
       "accuracy on class 0   1.000000   0.000000      0.486290\n",
       "accuracy on class 1   0.000000   1.000000      0.430556"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score Dataframe\n",
    "score_df = pd.DataFrame({'pos model': pos_model_scores,\n",
    "                         'neg model': neg_model_scores,\n",
    "                         'random model': random_model_scores})\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.059092642012962257"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build multible Models\n",
    "\n",
    "Now that we have an idea of how baseline models perform, let's try to improve upon them with fancier classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unweighted log\n",
      "unweighted quadratic log\n",
      "weighted log\n",
      "weighted log\n",
      "lda\n",
      "qda\n",
      "tree\n",
      "rf\n",
      "knn\n",
      "ada boost\n"
     ]
    }
   ],
   "source": [
    "# Unweighted logistic regression\n",
    "unweighted_logistic = LogisticRegression()\n",
    "unweighted_logistic.fit(x_train, y_train)\n",
    "\n",
    "unweighted_log_scores = score(unweighted_logistic, x_test, y_test)\n",
    "print 'unweighted log'\n",
    "\n",
    "# Unweighted quadtratic logistic regression\n",
    "unweighted__quad_logistic = LogisticRegression()\n",
    "unweighted__quad_logistic.fit(x_train_quad, y_train_quad)\n",
    "\n",
    "unweighted_quad_log_scores = score(unweighted__quad_logistic, x_test_quad, y_test_quad)\n",
    "print 'unweighted quadratic log'\n",
    "\n",
    "# Weighted logistic regression\n",
    "weighted_logistic = LogisticRegression(class_weight='balanced')\n",
    "weighted_logistic.fit(x_train, y_train)\n",
    "\n",
    "weighted_log_scores = score(weighted_logistic, x_test, y_test)\n",
    "print 'weighted log'\n",
    "\n",
    "# Weighted quadtratic logistic regression\n",
    "weighted_quad_logistic = LogisticRegression(class_weight='balanced')\n",
    "weighted_quad_logistic.fit(x_train_quad, y_train_quad)\n",
    "\n",
    "weighted_quad_log_scores = score(weighted_quad_logistic, x_test_quad, y_test_quad)\n",
    "print 'weighted log'\n",
    "\n",
    "#LDA\n",
    "lda = LDA()\n",
    "lda.fit(x_train, y_train)\n",
    "\n",
    "lda_scores = score(lda, x_test, y_test)\n",
    "print 'lda'\n",
    "\n",
    "#QDA\n",
    "qda = QDA()\n",
    "qda.fit(x_train, y_train)\n",
    "\n",
    "qda_scores = score(qda, x_test, y_test)\n",
    "print 'qda'\n",
    "\n",
    "#Decision Tree\n",
    "tree = DecisionTree(max_depth=30)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "tree_scores = score(tree, x_test, y_test)\n",
    "print 'tree'\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForest()\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "rf_scores = score(rf, x_test, y_test)\n",
    "\n",
    "print 'rf'\n",
    "\n",
    "#KNN\n",
    "knn = KNN(10)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "knn_scores = score(knn, x_test, y_test)\n",
    "print 'knn'\n",
    "\n",
    "# SVN\n",
    "svn = SVC(probability=True)\n",
    "svn.fit(x_train, y_train)\n",
    "\n",
    "svn_scores = score(svn, x_test, y_test)\n",
    "\n",
    "print 'svn'\n",
    "\n",
    "# SVN balanced\n",
    "svn_balanced = SVC(class_weight='balanced', probability=True)\n",
    "svn_balanced.fit(x_train, y_train)\n",
    "\n",
    "svn_balanced_scores = score(svn_balanced, x_test, y_test)\n",
    "\n",
    "print 'svn balanced'\n",
    "\n",
    "# Ada boost\n",
    "ada = ensemble.AdaBoostClassifier(n_estimators=20)\n",
    "ada.fit(x_train, y_train)\n",
    "\n",
    "ada_scores = score(ada, x_test, y_test)\n",
    "\n",
    "print 'ada boost'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ada boost</th>\n",
       "      <th>knn</th>\n",
       "      <th>lda</th>\n",
       "      <th>qda</th>\n",
       "      <th>rf</th>\n",
       "      <th>svn</th>\n",
       "      <th>svn balanced</th>\n",
       "      <th>tree</th>\n",
       "      <th>unweighted logistic</th>\n",
       "      <th>unweighted quadtratic logistic</th>\n",
       "      <th>weighted logistic</th>\n",
       "      <th>weighted quadtratic logistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall accuracy</th>\n",
       "      <td>0.945122</td>\n",
       "      <td>0.945122</td>\n",
       "      <td>0.945122</td>\n",
       "      <td>0.103659</td>\n",
       "      <td>0.941311</td>\n",
       "      <td>0.944360</td>\n",
       "      <td>0.912348</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.945884</td>\n",
       "      <td>0.943598</td>\n",
       "      <td>0.752287</td>\n",
       "      <td>0.710366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 0</th>\n",
       "      <td>0.990323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985484</td>\n",
       "      <td>0.054839</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.962903</td>\n",
       "      <td>0.940323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998387</td>\n",
       "      <td>0.762903</td>\n",
       "      <td>0.724194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ada boost       knn       lda       qda        rf  \\\n",
       "overall accuracy      0.945122  0.945122  0.945122  0.103659  0.941311   \n",
       "accuracy on class 0   0.990323  1.000000  0.985484  0.054839  0.995161   \n",
       "accuracy on class 1   0.166667  0.000000  0.250000  0.944444  0.013889   \n",
       "\n",
       "                          svn  svn balanced      tree  unweighted logistic  \\\n",
       "overall accuracy     0.944360      0.912348  0.899390             0.945884   \n",
       "accuracy on class 0  0.999194      0.962903  0.940323             1.000000   \n",
       "accuracy on class 1  0.000000      0.041667  0.194444             0.013889   \n",
       "\n",
       "                     unweighted quadtratic logistic  weighted logistic  \\\n",
       "overall accuracy                           0.943598           0.752287   \n",
       "accuracy on class 0                        0.998387           0.762903   \n",
       "accuracy on class 1                        0.000000           0.569444   \n",
       "\n",
       "                     weighted quadtratic logistic  \n",
       "overall accuracy                         0.710366  \n",
       "accuracy on class 0                      0.724194  \n",
       "accuracy on class 1                      0.472222  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score Dataframe\n",
    "score_df = pd.DataFrame({#'knn': knn_scores, \n",
    "                         'unweighted logistic': unweighted_log_scores,\n",
    "                         'weighted logistic': weighted_log_scores,\n",
    "                         'unweighted quadtratic logistic': unweighted_quad_log_scores,\n",
    "                         'weighted quadtratic logistic': weighted_quad_log_scores,\n",
    "                         'lda': lda_scores,\n",
    "                         'qda': qda_scores,\n",
    "                         'tree': tree_scores,\n",
    "                         'rf': rf_scores,\n",
    "                         'knn': knn_scores,\n",
    "                         'svn': svn_scores,\n",
    "                         'svn balanced': svn_balanced_scores,\n",
    "                         'ada boost': ada_scores})\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best ressults are achieved with the weighted logistic regression. We are trying to boost this a bit further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=20,\n",
       "       param_grid={'penalty': ['l2'], 'C': [0.0001, 0.1, 1, 10, 1000], 'solver': ['lbfgs', 'liblinear', 'sag']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': [0.0001, 0.1, 1, 10, 1000], 'solver' : ['lbfgs', 'liblinear', 'sag'] , 'penalty':['l2']}\n",
    " \n",
    "weighted_logistic_cv = LogisticRegression(penalty='l2',class_weight='balanced')\n",
    "all_grid_search_models = GridSearchCV(weighted_logistic_cv, param_grid=parameters, cv=5, n_jobs=20, scoring='f1')\n",
    "all_grid_search_models.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best C value :', 0.1)\n",
      "('Best solver value :', 'liblinear')\n",
      "('Best penalty value :', 'l2')\n",
      "('Accuracy on training data :', 0.75597356380274527)\n",
      "('Accuracy on test data     :', 0.74923780487804881)\n"
     ]
    }
   ],
   "source": [
    "# Best estimator \n",
    "weighted_logistic_cv = all_grid_search_models.best_estimator_\n",
    "\n",
    "# Accuracy\n",
    "training_accuracy = weighted_logistic_cv.score(x_train, y_train)\n",
    "test_accuracy = weighted_logistic_cv.score(x_test, y_test)\n",
    "\n",
    "print (\"Best C value :\" , weighted_logistic_cv.C )\n",
    "print (\"Best solver value :\" , weighted_logistic_cv.solver)\n",
    "print (\"Best penalty value :\" , weighted_logistic_cv.penalty)\n",
    "print (\"Accuracy on training data :\" , training_accuracy )\n",
    "print (\"Accuracy on test data     :\" , test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Scoring function\n",
    "def cutoff_predict(clf, X, threshold_1):\n",
    "    y_prob = clf.predict_proba(X)\n",
    "    y_pred = np.zeros((X.shape[0], 1)).reshape(-1, )\n",
    "    y_pred[y_prob[:,1] > threshold_1] = 1\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def custom_f1(threshold_1):\n",
    "    def f1_cutoff(clf, X, y):\n",
    "        ypred = cutoff_predict(clf, X, threshold_1)\n",
    "        return metrics.f1_score(y, ypred)\n",
    "        \n",
    "    return f1_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[944 296]\n",
      " [ 33  39]]\n",
      "overall accuracy       0.749238\n",
      "accuracy on class 0    0.761290\n",
      "accuracy on class 1    0.541667\n",
      "dtype: float64\n",
      "[[894 346]\n",
      " [ 27  45]]\n",
      "Accuray for Class 0 =  0\n",
      "Accuray for Class 1 =  0\n"
     ]
    }
   ],
   "source": [
    "# Weighted logistic_cv\n",
    "weighted_logistic_cv_scores = score(weighted_logistic_cv, x_test, y_test)\n",
    "print confusion_matrix(y_test, weighted_logistic_cv.predict(x_test))\n",
    "print weighted_logistic_cv_scores\n",
    "\n",
    "y_pred = cutoff_predict(weighted_logistic_cv,x_test,0.467)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print cm\n",
    "print 'Accuray for Class 0 = ' ,cm[0, 0]/sum(cm[0, :])\n",
    "print 'Accuray for Class 1 = ' ,cm[1, 1]/sum(cm[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unweighted logistic regression\n",
    "un_log_pred = unweighted_logistic.predict_proba(x_train)\n",
    "un_quad_log_pred = unweighted__quad_logistic.predict_proba(x_train_quad)\n",
    "w_log_pred = weighted_logistic.predict_proba(x_train)\n",
    "w_quad_log_pred = weighted_quad_logistic.predict_proba(x_train_quad)\n",
    "lda_pred = lda.predict_proba(x_train)\n",
    "qda_pred = qda.predict_proba(x_train)\n",
    "tree_pred = tree.predict_proba(x_train)\n",
    "rf_pred = rf.predict_proba(x_train)\n",
    "knn_pred = knn.predict_proba(x_train)\n",
    "svn_pred = svn.predict_proba(x_train)\n",
    "svn_b_pred = svn_balanced.predict_proba(x_train)\n",
    "ada_pred = ada.predict_proba(x_train)\n",
    "\n",
    "# Build matrix\n",
    "pred_train = np.hstack([un_log_pred, un_quad_log_pred, w_log_pred, w_quad_log_pred, lda_pred, qda_pred, tree_pred, rf_pred, knn_pred, svn_pred, svn_b_pred, ada_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unweighted logistic regression\n",
    "un_log_pred = unweighted_logistic.predict_proba(x_test)\n",
    "un_quad_log_pred = unweighted__quad_logistic.predict_proba(x_test_quad)\n",
    "w_log_pred = weighted_logistic.predict_proba(x_test)\n",
    "w_quad_log_pred = weighted_quad_logistic.predict_proba(x_test_quad)\n",
    "lda_pred = lda.predict_proba(x_test)\n",
    "qda_pred = qda.predict_proba(x_test)\n",
    "tree_pred = tree.predict_proba(x_test)\n",
    "rf_pred = rf.predict_proba(x_test)\n",
    "knn_pred = knn.predict_proba(x_test)\n",
    "svn_pred = svn.predict_proba(x_test)\n",
    "svn_b_pred = svn_balanced.predict_proba(x_test)\n",
    "ada_pred = ada.predict_proba(x_test)\n",
    "\n",
    "# Build matrix\n",
    "pred_test = np.hstack([un_log_pred, un_quad_log_pred, w_log_pred, w_quad_log_pred, lda_pred, qda_pred, tree_pred, rf_pred, knn_pred, svn_pred, svn_b_pred, ada_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89939024390243905"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic ensemble model\n",
    "log_model = LogisticRegression().fit(pred_train, y_train)\n",
    "test_y_pred = log_model.predict(pred_test)\n",
    "log_model_accuracy = np.mean(test_y_pred == y_test)\n",
    "log_model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall accuracy       0.899390\n",
       "accuracy on class 0    0.940323\n",
       "accuracy on class 1    0.194444\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_score = score(log_model, pred_test, y_test)\n",
    "log_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like that the weighted logistic regression model with linear factors is still the best model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deliverable: a function called `flu_predict` which satisfies:\n",
    "\n",
    "- input: `x_test`, a set of medical predictors for a group of patients\n",
    "- output: `y_pred`, a set of labels, one for each patient; 0 for healthy and 1 for infected with the flu virus\n",
    "\n",
    "The MA state government will use your model to diagnose sets of future patients (held by us). You can expect that there will be an increase in the number of flu patients in any groups of patients in the future.\n",
    "\n",
    "We provide you with some benchmarks for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 919.  321.]\n",
      " [  28.   44.]]\n",
      "Accuray for Class 0 =  0.741129032258\n",
      "Accuray for Class 1 =  0.611111111111\n"
     ]
    }
   ],
   "source": [
    "def flu_predict(x_test):\n",
    "    log_model = LogisticRegression(C=10, solver='liblinear', penalty='l2', class_weight='balanced').fit(x_train, y_train)\n",
    "    test_y_pred = log_model.predict_proba(x_test)\n",
    "    y_pred = pd.DataFrame(test_y_pred)[1] \n",
    "    y_pred[y_pred >= 0.48] = 1\n",
    "    y_pred[y_pred < 0.52] = 0\n",
    "    return y_pred\n",
    "\n",
    "y_pred = flu_predict(x_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype(float)\n",
    "print cm\n",
    "print 'Accuray for Class 0 = ' ,cm[0,0]/sum(cm[0,:])\n",
    "print 'Accuray for Class 1 = ' ,cm[1,1]/sum(cm[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_flu_fin = pd.read_csv('datasets/flu_test_no_y.csv')\n",
    "y_pred = pd.Series(flu_predict(x_fin))\n",
    "df_pred = pd.DataFrame({\"id\": df_flu_fin.ID, \"pred\": y_pred})\n",
    "df_pred.to_csv(\"datasets/pred1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Diagnosing Strains of the Semian Flu\n",
    "\n",
    "From a public health perspective, we want to balance the cost of vaccinations, early interventions and the cost of treating flu complications of unvaccinated people. \n",
    "\n",
    "There are two different strains of the flu: strain 1 has a cheaper early intervention as well as a cheaper treatment for flu complications, but patients with strain 1 has a higher rate of developing complications if treated with the wrong intervention. Strain 2 has a more expensive early intervention as well as a more costly treatment for flu complications, but patients with strain 2 has a lower rate of developing complications if treated with the wrong intervention. With no intervention, flu patients develop complications at the same rate regardless of the strain. \n",
    "\n",
    "**Your task:** build a model to predict if a given patient has the flu and identify the flu strain. The state government of MA will use your model to inform public health policies: we will vaccinate people you've identified as healthy and apply corresponding interventions to patients with different strains of the flu. We have provided you with a function to compute the total expected cost of this policy decision that takes into account the cost of the vaccine, the interventions and the cost of the treatments for flu complications resulting from misdiagnosing patients. Your goal is to make sure your model produces a public health policy with the lowest associated expected cost.\n",
    "\n",
    "**The deliverable:** a function called `flu_predict` which satisfies:\n",
    "\n",
    "- input: `x_test`, a set of medical predictors for a group of patients\n",
    "- output: `y_pred`, a set of labels, one for each patient; 1 for healthy, 2 for infected with strain 1, and 3 for infected with strain 2.\n",
    "\n",
    "The MA state government will use your model to diagnose sets of future patients (held by us). You can expect that there will be an increase in the number of flu patients in any groups of patients in the future.\n",
    "\n",
    "We provide you with some benchmarks for comparison.\n",
    "\n",
    "**Three Baseline Models:** \n",
    "- expected cost on observed data: \\$6,818,206.0, \\$7,035,735.0, \\$8,297,197.5\n",
    "- time to build: 1 min\n",
    "\n",
    "**Reasonable Model:** \n",
    "- expected cost on observed data: $6,300,000\n",
    "- time to build: 20 min\n",
    "\n",
    "**Grading:**\n",
    "Your grade will be based on:\n",
    "1. your model's ability to out-perform our benchmarks\n",
    "2. your ability to carefully and thoroughly follow the data science pipeline (see lecture slides for definition)\n",
    "3. the extend to which all choices are reasonable and defensible by methods you have learned in this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  cost\n",
    "# A function that computes the expected cost of the public healthy policy based on the \n",
    "# classifications generated by your model\n",
    "# Input: \n",
    "#      y_true (true class labels: 0, 1, 2)\n",
    "#      y_pred (predicted class labels: 0, 1, 2)\n",
    "# Returns: \n",
    "#      total_cost (expected total cost)\n",
    "\n",
    "def cost(y_true, y_pred):\n",
    "    cost_of_treatment_1 = 29500\n",
    "    cost_of_treatment_2 = 45000\n",
    "    cost_of_intervention_1 = 4150\n",
    "    cost_of_intervention_2 = 4250\n",
    "    cost_of_vaccine = 15\n",
    "    \n",
    "    prob_complications_untreated = 0.65\n",
    "    prob_complications_1 = 0.30\n",
    "    prob_complications_2 = 0.15\n",
    "    \n",
    "    trials = 1000\n",
    "    \n",
    "    \n",
    "    intervention_cost = cost_of_intervention_1 * len(y_pred[y_pred==1]) + cost_of_intervention_2 * len(y_pred[y_pred==2])\n",
    "\n",
    "    vaccine_cost = cost_of_vaccine * len(y_pred[y_pred==0])\n",
    "    \n",
    "    false_neg_1 = ((y_true == 1) & (y_pred == 2)).sum()\n",
    "    false_neg_2 = ((y_true == 2) & (y_pred == 1)).sum()\n",
    "    \n",
    "    untreated_1 = ((y_true == 1) & (y_pred == 0)).sum()    \n",
    "    untreated_2 = ((y_true == 2) & (y_pred == 0)).sum()\n",
    "    \n",
    "    false_neg_1_cost = np.random.binomial(1, prob_complications_1, (false_neg_1, trials)) * cost_of_treatment_1\n",
    "    false_neg_2_cost = np.random.binomial(1, prob_complications_2, (false_neg_2, trials)) * cost_of_treatment_2\n",
    "    untreated_1_cost = np.random.binomial(1, prob_complications_untreated, (untreated_1, trials)) * cost_of_treatment_1\n",
    "    untreated_2_cost = np.random.binomial(1, prob_complications_untreated, (untreated_2, trials)) * cost_of_treatment_2\n",
    "    \n",
    "    false_neg_1_cost = false_neg_1_cost.sum(axis=0)\n",
    "    expected_false_neg_1_cost = false_neg_1_cost.mean()\n",
    "    \n",
    "    false_neg_2_cost = false_neg_2_cost.sum(axis=0)\n",
    "    expected_false_neg_2_cost = false_neg_2_cost.mean()\n",
    "    \n",
    "    untreated_1_cost = untreated_1_cost.sum(axis=0)\n",
    "    expected_untreated_1_cost = untreated_1_cost.mean()\n",
    "    \n",
    "    untreated_2_cost = untreated_2_cost.sum(axis=0)\n",
    "    expected_untreated_2_cost = untreated_2_cost.mean()\n",
    "    \n",
    "    total_cost = vaccine_cost + intervention_cost + expected_false_neg_1_cost + expected_false_neg_2_cost + expected_untreated_1_cost + expected_untreated_2_cost\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234) # Set random seed\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y2, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that new train set and validation set has the same distribuition as orginal train set\n"
     ]
    }
   ],
   "source": [
    "print 'Check that new train set and validation set has the same distribuition as orginal train set'\n",
    "\n",
    "#  get the balance of flutype, a 1 indicates the absences of the virus, a 1 indicates presence of strain 1 and a 2 indicates the presence of strain 2).\n",
    "#Get the balance for each class \n",
    "n_train = len(y_train)\n",
    "flutype_1_in_train = len(y_train[y_train == 1])\n",
    "flutype_2_in_train = len(y_train[y_train == 2])\n",
    "flutype_3_in_train = len(y_train[y_train == 3])\n",
    "\n",
    "n_test_val = len(y_test)\n",
    "flutype_1_in_test = len(y_test[y_test == 1])\n",
    "flutype_2_in_test = len(y_test[y_test == 2])\n",
    "flutype_3_in_test = len(y_test[y_test == 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 1 in -- Train 3696  out of 3934 ( 0 %) *** Test  1240  out of  1312 ( 0 %)\n",
      "Type 2 in -- Train 174  out of 3934 ( 0 %) *** Test  53  out of  1312 ( 0 %)\n",
      "Type 3 in -- Train 64  out of 3934 ( 0 %) *** Test  19  out of  1312 ( 0 %)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAHpCAYAAADQy56BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x184db978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print 'Type 1 in -- Train',flutype_1_in_train , ' out of' ,n_train , '(' , np.round(flutype_1_in_train/n_train,2) * 100, '%) *** Test ',flutype_1_in_test,' out of ',n_test_val, '(',np.round(flutype_1_in_test/n_test_val,2)* 100, '%)'\n",
    "print 'Type 2 in -- Train',flutype_2_in_train , ' out of' ,n_train , '(' , np.round(flutype_2_in_train/n_train,2) * 100, '%) *** Test ',flutype_2_in_test,' out of ',n_test_val, '(',np.round(flutype_2_in_test/n_test_val,2)* 100, '%)'\n",
    "print 'Type 3 in -- Train',flutype_3_in_train , ' out of' ,n_train , '(' , np.round(flutype_3_in_train/n_train,2) * 100, '%) *** Test ',flutype_3_in_test,' out of ',n_test_val, '(',np.round(flutype_3_in_test/n_test_val,2)* 100, '%)'\n",
    "\n",
    "# Display the distribution of response variable in the train data set\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "ax.hist(y_train, color=\"darkred\", alpha=0.9, bins = 5, edgecolor=\"none\")\n",
    "ax.hist(y_test, color=\"darkblue\", alpha=0.9, bins = 5, edgecolor=\"none\")\n",
    "ax.set_title('Distribution of the target variable y')\n",
    "ax.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for computing the accuracy a given model on the entire test set, \n",
    "#the accuracy on class 0 \n",
    "#the accuracy on class 1\n",
    "def score2(model,x_test,y_test):\n",
    "    overall_accuracy = model.score(x_test, y_test)\n",
    "    accuracy_on_class_1 = model.score(x_test[y_test==1], y_test[y_test==1])\n",
    "    accuracy_on_class_2 = model.score(x_test[y_test==2], y_test[y_test==2])\n",
    "    accuracy_on_class_3 = model.score(x_test[y_test==3], y_test[y_test==3])\n",
    "\n",
    "    return  pd.Series([overall_accuracy,  accuracy_on_class_1,accuracy_on_class_2 ,accuracy_on_class_3],\n",
    "               index=['overall accuracy', 'accuracy on class 1', 'accuracy on class 2','accuracy on class 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A model that labels everything 1\n",
    "class Basic_model1(object):\n",
    "    def predict(self, x):\n",
    "        return np.array([1] * len(x))\n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)\n",
    "    \n",
    "# A model that labels everything 2\n",
    "class Basic_model2(object):\n",
    "    def predict(self, x):\n",
    "        return np.array([2] * len(x))\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)\n",
    "\n",
    "\n",
    "# A model that labels everything 3\n",
    "class Basic_model3(object):\n",
    "    def predict(self, x):\n",
    "        return np.array([3] * len(x))\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)\n",
    "                         \n",
    "# A model that randomly labels things\n",
    "class Random_model(object):\n",
    "    def predict(self, x):\n",
    "        return np.random.randint(0, 4, len(x))\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basic_model1</th>\n",
       "      <th>basic_model2</th>\n",
       "      <th>basic_model3</th>\n",
       "      <th>random model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall accuracy</th>\n",
       "      <td>0.945122</td>\n",
       "      <td>0.040396</td>\n",
       "      <td>0.014482</td>\n",
       "      <td>0.230945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.157895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     basic_model1  basic_model2  basic_model3  random model\n",
       "overall accuracy         0.945122      0.040396      0.014482      0.230945\n",
       "accuracy on class 1      1.000000      0.000000      0.000000      0.251613\n",
       "accuracy on class 2      0.000000      1.000000      0.000000      0.245283\n",
       "accuracy on class 3      0.000000      0.000000      1.000000      0.157895"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_model1 = Basic_model1()\n",
    "basic_model1_scores = score2(basic_model1, x_test, y_test)\n",
    "\n",
    "basic_model2 = Basic_model2()\n",
    "basic_model2_scores = score2(basic_model2, x_test, y_test)\n",
    "\n",
    "basic_model3 = Basic_model3()\n",
    "basic_model3_scores = score2(basic_model3, x_test, y_test)\n",
    "\n",
    "random_model = Random_model()\n",
    "random_model_scores = score2(random_model, x_test, y_test)\n",
    "\n",
    "#Score Dataframe\n",
    "score_df = pd.DataFrame({'basic_model1': basic_model1_scores,\n",
    "                         'basic_model2': basic_model2_scores,\n",
    "                         'basic_model3': basic_model3_scores,\n",
    "                         'random model': random_model_scores})\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models\n",
    "\n",
    "Note  :  Ignore KNN \n",
    "    as the problem state that \"The MA state government will use your model to diagnose sets of future patients (held by us). You can expect that there will be an increase in the number of flu patients in any groups of patients in the future.\"\n",
    "    \n",
    "    And As the KNN take very long time in prediction so this model will not be suitbale in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flutype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>4936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     flutype\n",
       "1.0     4936\n",
       "2.0      227\n",
       "3.0       83"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table(df_flu.flutype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unweighted log\n",
      "unweighted quadratic log\n",
      "weighted log\n",
      "weighted log\n",
      "lda\n",
      "qda\n",
      "rf\n",
      "ada boost\n"
     ]
    }
   ],
   "source": [
    "# Unweighted logistic regression\n",
    "unweighted_logistic = LogisticRegression()\n",
    "unweighted_logistic.fit(x_train, y_train)\n",
    "\n",
    "unweighted_log_scores = score2(unweighted_logistic, x_test, y_test)\n",
    "print 'unweighted log'\n",
    "\n",
    "# Unweighted quadtratic logistic regression\n",
    "unweighted__quad_logistic = LogisticRegression()\n",
    "unweighted__quad_logistic.fit(x_train_quad, y_train_quad)\n",
    "\n",
    "unweighted_quad_log_scores = score2(unweighted__quad_logistic, x_test_quad, y_test)\n",
    "print 'unweighted quadratic log'\n",
    "\n",
    "# Weighted logistic regression\n",
    "weighted_logistic = LogisticRegression(class_weight='balanced')\n",
    "weighted_logistic.fit(x_train, y_train)\n",
    "\n",
    "weighted_log_scores = score2(weighted_logistic, x_test, y_test)\n",
    "print 'weighted log'\n",
    "\n",
    "# Weighted quadtratic logistic regression\n",
    "weighted_quad_logistic = LogisticRegression(class_weight='balanced')\n",
    "weighted_quad_logistic.fit(x_train_quad, y_train_quad)\n",
    "\n",
    "weighted_quad_log_scores = score2(weighted_quad_logistic, x_test_quad, y_test)\n",
    "print 'weighted log'\n",
    "\n",
    "#LDA\n",
    "lda = LDA()\n",
    "lda.fit(x_train, y_train)\n",
    "\n",
    "lda_scores = score2(lda, x_test, y_test)\n",
    "print 'lda'\n",
    "\n",
    "#QDA\n",
    "qda = QDA()\n",
    "qda.fit(x_train, y_train)\n",
    "\n",
    "qda_scores = score2(qda, x_test, y_test)\n",
    "print 'qda'\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForest()\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "rf_scores = score2(rf, x_test, y_test)\n",
    "\n",
    "print 'rf'\n",
    "\n",
    "# Ada boost\n",
    "ada = ensemble.AdaBoostClassifier(n_estimators=20)\n",
    "ada.fit(x_train, y_train)\n",
    "\n",
    "ada_scores = score2(ada, x_test, y_test)\n",
    "\n",
    "print 'ada boost'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = timeit.timeit()\n",
    "svm = SVC(C=1000, kernel='linear', class_weight='balanced')\n",
    "svm.fit(x_train, y_train)\n",
    "svm_scores = score2(svm, x_test, y_test)\n",
    "end = timeit.timeit()\n",
    "\n",
    "print  'svm Weighted Linear'\n",
    "\n",
    "svm_time = pd.Series([start - end], index=['Model Fit Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ada boost</th>\n",
       "      <th>lda</th>\n",
       "      <th>qda</th>\n",
       "      <th>rf</th>\n",
       "      <th>unweighted logistic</th>\n",
       "      <th>unweighted quadtratic logistic</th>\n",
       "      <th>weighted logistic</th>\n",
       "      <th>weighted quadtratic logistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall accuracy</th>\n",
       "      <td>0.942073</td>\n",
       "      <td>0.933689</td>\n",
       "      <td>0.089939</td>\n",
       "      <td>0.947409</td>\n",
       "      <td>0.945122</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.912348</td>\n",
       "      <td>0.260671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 1</th>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.957258</td>\n",
       "      <td>0.275806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 2</th>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 3</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ada boost       lda       qda        rf  \\\n",
       "overall accuracy      0.942073  0.933689  0.089939  0.947409   \n",
       "accuracy on class 1   0.994355  0.983871  0.056452  0.999194   \n",
       "accuracy on class 2   0.018868  0.018868  0.905660  0.056604   \n",
       "accuracy on class 3   0.105263  0.210526  0.000000  0.052632   \n",
       "\n",
       "                     unweighted logistic  unweighted quadtratic logistic  \\\n",
       "overall accuracy                0.945122                        0.001524   \n",
       "accuracy on class 1             1.000000                        0.001613   \n",
       "accuracy on class 2             0.000000                        0.000000   \n",
       "accuracy on class 3             0.000000                        0.000000   \n",
       "\n",
       "                     weighted logistic  weighted quadtratic logistic  \n",
       "overall accuracy              0.912348                      0.260671  \n",
       "accuracy on class 1           0.957258                      0.275806  \n",
       "accuracy on class 2           0.075472                      0.000000  \n",
       "accuracy on class 3           0.315789                      0.000000  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score Dataframe\n",
    "score_df = pd.DataFrame({'unweighted logistic': unweighted_log_scores,\n",
    "                         'weighted logistic': weighted_log_scores,\n",
    "                         'unweighted quadtratic logistic': unweighted_quad_log_scores,\n",
    "                         'weighted quadtratic logistic': weighted_quad_log_scores,\n",
    "                         #'SVM Weighted Linear': svm_scores,\n",
    "                         'lda': lda_scores,\n",
    "                         'qda': qda_scores,\n",
    "                         'rf': rf_scores,\n",
    "                         'ada boost': ada_scores})\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from the above test it seem that the SVM weighted Linear can be used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Three Baseline Models:** \n",
    "- expected cost on observed data: \\$6,818,206.0, \\$7,035,735.0, \\$8,297,197.5\n",
    "- time to build: 1 min\n",
    "\n",
    "**Reasonable Model:** \n",
    "- expected cost on observed data: $6,300,000\n",
    "- time to build: 20 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline 1: 25364732.0\n",
      "baseline 2: 5808040.0\n",
      "baseline 3: 16527344.0\n"
     ]
    }
   ],
   "source": [
    "print \"baseline 1:\", cost(y_test, np.repeat(0, len(y_test)))\n",
    "print \"baseline 2:\", cost(y_test, np.repeat(1, len(y_test)))\n",
    "print \"baseline 3:\", cost(y_test, np.repeat(2, len(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that I don't get to the same expeced costs as shown above. My models indicates, that doing nothing, would cost around 25 million Dollars, giving everybody the second early intervention would cost around 5.8 million dollars and giving everybody the more expensive intervention would cost around 16.5 million dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.945122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.040396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.014482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "1.0  0.945122\n",
       "2.0  0.040396\n",
       "3.0  0.014482"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table(pd.Series(y_test)) / len(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1493444.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flu_predict(x_test):\n",
    "    log_model = LogisticRegression(C=10, solver='liblinear', penalty='l2', class_weight='balanced').fit(x_train, y_train)\n",
    "    test_y_pred = log_model.predict(x_test)\n",
    "    y_pred = pd.DataFrame(test_y_pred)[0] \n",
    "    return y_pred\n",
    "\n",
    "y_pred = flu_predict(x_test)\n",
    "\n",
    "y_pred = y_pred - 1\n",
    "y_true = y_test - 1\n",
    "cost(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logisitc regression model only costs 1.5 Million which brings a lot of savings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-57b23c43bebe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_flu_fin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'datasets/flu_test_no_y.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflu_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_fin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdf_flu_fin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pred\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"datasets/pred2.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_flu_fin = pd.read_csv('datasets/flu_test_no_y.csv')\n",
    "y_pred = pd.Series(flu_predict(x_fin))\n",
    "df_pred = pd.DataFrame({\"id\": df_flu_fin.ID, \"pred\": y_pred})\n",
    "df_pred.to_csv(\"datasets/pred2.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
